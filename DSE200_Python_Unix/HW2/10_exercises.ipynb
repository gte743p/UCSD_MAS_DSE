{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 Exercises\n",
    "\n",
    "In this weeks exercises you will use Numpy/Scipy to impliment some numerical algorithms and then you will use Pandas to perform a rudamentary data analysis using the KDD 98 dataset.  Along the way you will use unix/basic python from the first week as well as git to save your work.\n",
    "\n",
    "As a first step we import the libraries we'll use later on.  This allows us to use numpy library calls by prefixing the call with np."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import the libraries \n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Manipulations\n",
    "Lets first create a matrix and perform some manipulations of it.\n",
    "\n",
    "Using numpy's matrix data structure, define the following matricies:\n",
    "\n",
    "$$A=\\left[ \\begin{array}{ccc} 3 & 5 & 9 \\\\ 3 & 3 & 4 \\\\ 5 & 9 & 17 \\end{array} \\right]$$\n",
    "\n",
    "$$B=\\left[ \\begin{array}{c} 2 \\\\ 1 \\\\ 4 \\end{array} \\right]$$\n",
    "\n",
    "After this solve the matrix equation:\n",
    "$$Ax = B$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write three functions for matrix multiply $C=AB$ in each of the following styles:\n",
    "\n",
    "1. By using nested for loops to impliment the naive algorithm ($C_{ij}=\\sum_{k=0}^{m-1}A_{ik}B_{kj}$)\n",
    "2. Using numpy's built in martrix multiplication  \n",
    "3. Using Cython\n",
    "\n",
    "The three methods should have the same answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define A and B\n",
    "A = [[3, 5, 9], \n",
    "     [3, 3, 4], \n",
    "     [5, 9, 17]]\n",
    "\n",
    "B = [[2], \n",
    "     [1], \n",
    "     [4]]\n",
    "\n",
    "# solve Ax = B using numpy matrix multiplication\n",
    "x = np.matrix(A).I * np.matrix(B)\n",
    "print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# solving C = AB using nested for loops\n",
    "def matrix_mult_naive(A, B):\n",
    "    C = []\n",
    "    # iterate over rows of A\n",
    "    for i in range(len(A)):\n",
    "        C_row = []\n",
    "        # iterate over columns of B\n",
    "        for j in range(len(B[0])):\n",
    "            # iteratively sum pairwise elements in ith row of A and jth column of B\n",
    "            indexSum = 0\n",
    "            for k in range(len(A[i])):\n",
    "                indexSum += A[i][k]*B[k][j]\n",
    "            C_row.append(indexSum)\n",
    "        C.append(C_row)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# test cell for multiplying large matrixes using naive python\n",
    "tA = [[np.random.uniform() for x in range(100)] for y in range(1000)]\n",
    "tB = [[np.random.uniform() for x in range(1000)] for y in range(100)]\n",
    "\n",
    "matrix_mult_naive(tA, tB)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to multiply matrices using numpy\n",
    "def matrix_mult_np(A, B):\n",
    "    return np.matrix(A) * np.matrix(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "cimport numpy as np\n",
    "import numpy as np\n",
    "\n",
    "# solving C = AB using cython\n",
    "def matrix_mult_cython(np.ndarray[np.int64_t, ndim=2] A, np.ndarray[np.int64_t, ndim=2] B):\n",
    "    # TODO\n",
    "    #cdef npc.ndarray[npc.int64_t, ndim=2] C\n",
    "    cdef np.ndarray[np.int64_t, ndim=2] C = np.empty((A.shape[0], B.shape[1]), dtype=np.int64)\n",
    "    cdef np.ndarray[np.int64_t, ndim=1] C_row = np.empty(B.shape[1], dtype=np.int64)\n",
    "    cdef long i\n",
    "    cdef long j\n",
    "    cdef long k\n",
    "    cdef double indexSum\n",
    "    \n",
    "    # iterate over rows of A\n",
    "    for i in xrange(A.shape[0]):\n",
    "        # iterate over columns of B\n",
    "        for j in xrange(B.shape[1]):\n",
    "            # iteratively sum pairwise elements in ith row of A and jth column of B\n",
    "            indexSum = 0\n",
    "            for k in xrange(A[i].shape[0]):\n",
    "                indexSum += A[i][k]*B[k][j]\n",
    "            #C_row.append(indexSum)\n",
    "            C[i][j] = indexSum\n",
    "        #C.append(C_row)\n",
    "    return C\n",
    "    #return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# show that all three methods produce the same result\n",
    "\n",
    "# basic python matrix multiplication\n",
    "print matrix_mult_naive(A, B)\n",
    "print '\\n'\n",
    "\n",
    "# numpy matrix multiplication\n",
    "print np.matrix(A) * np.matrix(B)\n",
    "print '\\n'\n",
    "\n",
    "# cython matrix multiplication\n",
    "print matrix_mult_cython(np.array(A), np.array(B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we wish to evaluate the performance of these three methods.  Write a method that given three dmiensions (a,b,c) makes a random a x b and b x c matrix and computes the product using your three functions and reports the speed of each method.\n",
    "\n",
    "After this measure performance of each method for all $a,b,c \\in \\{10,100,1000,10000\\}$ and plot the results.  Is one method always the fastest?  Discuss why this is or is not the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# function to determine time to multiply two matrices of size a x b and b x c\n",
    "def rand_matrix_prod(a, b, c):\n",
    "    # create m1 and m2, random matrices of dimension a x b and b x c, respecively\n",
    "    m1 = [[np.random.randint(5) for x in range(b)] for y in range(a)]\n",
    "    m2 = [[np.random.randint(5) for x in range(c)] for y in range(b)]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    matrix_mult_naive(m1, m2)\n",
    "    naive_time = time.time()\n",
    "    matrix_mult_np(np.matrix(m1), np.matrix(m2))\n",
    "    numpy_time = time.time()\n",
    "    matrix_mult_cython(np.array(m1), np.array(m2))\n",
    "    cython_time = time.time()\n",
    "    \n",
    "    digits = 8\n",
    "    \n",
    "    naive_dur = round(naive_time - start_time, digits)\n",
    "    numpy_dur = round(numpy_time - naive_time, digits)\n",
    "    cython_dur = round(cython_time - numpy_time, digits)\n",
    "    \n",
    "    return [naive_dur, numpy_dur, cython_dur]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create list of inputs\n",
    "s = [10, 100]#, 1000, 10000]\n",
    "inputs = [[a, b, c] for a in s for b in s for c in s]\n",
    "\n",
    "# run rand_matrix_prod function to time execution of matrix multiplications\n",
    "results_prod = [rand_matrix_prod(input[0], input[1], input[2]) for input in inputs]\n",
    "\n",
    "# print matrix size inputs and list of times required for multiplying with python, numpy, and cython\n",
    "inputs, results_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create plots of matrix multiplication timing results\n",
    "\n",
    "# isolate results from each method\n",
    "results_prod_transpose = np.array(results_prod).T\n",
    "python_results_prod = results_prod_transpose[0]\n",
    "numpy_results_prod = results_prod_transpose[1]\n",
    "cython_results_prod = results_prod_transpose[2]\n",
    "\n",
    "# boxplot of results\n",
    "bp = plt.boxplot(np.array(results_prod))\n",
    "\n",
    "## change the style of fliers and their fill\n",
    "for flier in bp['fliers']:\n",
    "    flier.set(marker='o', color='#e7298a', alpha=0.5)\n",
    "\n",
    "# add labels to boxplot\n",
    "plt.title('Matrix Multiplication Execution Time by Method')\n",
    "plt.ylabel('Seconds')\n",
    "plt.xticks([1, 2, 3], ['Python', 'Numpy', 'Cython'])\n",
    "\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# create plot by experiment number, which corresponds to 'inputs' printed in the previous cell\n",
    "plt.title('Matrix Multiplication Execution Time by Method')\n",
    "plt.ylabel('Seconds')\n",
    "plt.xlabel('Experiment #')\n",
    "\n",
    "# re-index values so their index corresponds to experiment number\n",
    "plt.plot(np.append(np.roll(python_results_prod,1),python_results_prod[-1]), 'r--', label = 'Python', alpha = 0.75)\n",
    "plt.plot(np.append(np.roll(numpy_results_prod,1),numpy_results_prod[-1]), 'bs', label = 'Numpy', alpha = 0.75)\n",
    "plt.plot(np.append(np.roll(cython_results_prod,1),cython_results_prod[-1]), 'g^', label = 'Cython', alpha = 0.75)\n",
    "plt.legend(loc = 'upper left')\n",
    "\n",
    "# reset xticks to correspond to experiment number\n",
    "ax = plt.subplot(111)\n",
    "size = len(python_results_prod) + 1\n",
    "ax.set_xlim(1, size - 1)\n",
    "dim = np.arange(1, size, 1)\n",
    "plt.xticks(dim)\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of Matrix Multiplication Timing Results\n",
    "It appears that the time required to calculate the product of a matrices using base python and cython increase as the size of the matrices increase, while the time required to calculate the product using numpy is not significantly increased as matrix size grows.\n",
    "\n",
    "For example, calculating the product of a random square matrices of size 100 using naive python requires roughly 0.31 seconds, and cython requires roughly 0.4 secondson my machine, but the time required using numpy is approximately 0.02 seconds.  In all cases tested, numpy results in a faster calculation.\n",
    "\n",
    "The reason for numpy's relative speed is that it is a specialized package that is optimized to perform calculations on arrays that must consist of elements of the same type, while python lists are arrays of pointers to objects, even if all the elements are of the same type, as in this case.  The implication of this difference is that python arrays require more space to store and take longer to access from memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BONUS** Now repeat the past two problems but instead of computing the matrix product, compute a matrix's [determinant](http://en.wikipedia.org/wiki/Determinant).  Measure performance for matricies of various sizes and discuss the results.  Determinant may get impractical to calculate for not too huge of matricies, so no need to goto 1000x1000 matricies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to calculate the determinant of a matrix using naive python\n",
    "def matrix_det_naive(A):\n",
    "    # base cases:  A is 1x1 or 2x2\n",
    "    if len(A) == 1:\n",
    "        det = A[0][0]\n",
    "    elif len(A) == 2:\n",
    "        det = A[0][0]*A[1][1] - A[0][1]*A[1][0]\n",
    "    else:\n",
    "        det = 0\n",
    "        # calculate determinant by minors, iterating over first row of A\n",
    "        # see http://mathworld.wolfram.com/DeterminantExpansionbyMinors.html\n",
    "        for i in range(len(A[0])):\n",
    "            minor = [row[0:i] + row[i+1:] for row in A[1:]]\n",
    "            det += (-1)**(i+2) * A[0][i] * matrix_det_naive(minor)\n",
    "    return det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# test determinant function for random matrix\n",
    "size = 8\n",
    "m = [[np.random.randint(10) for _ in range(size)] for _ in range(size)]\n",
    "\n",
    "print np.linalg.det(np.array(m))\n",
    "print matrix_det_naive(m)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to determine time required to calculate determinant of matrix of size n using python and numpy\n",
    "def rand_matrix_det(n):\n",
    "    # create random nxn matrix named m1\n",
    "    m1 = [[np.random.randint(5) for _ in range(n)] for _ in range(n)]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    matrix_det_naive(m1)\n",
    "    naive_time = time.time()\n",
    "    np.linalg.det(np.array(m1))\n",
    "    numpy_time = time.time()\n",
    "    #matrix_det_cython(np.array(m1))\n",
    "    #cython_time = time.time()\n",
    "    \n",
    "    # round time required to 8 digits\n",
    "    digits = 8\n",
    "    naive_dur = round(naive_time - start_time, digits)\n",
    "    numpy_dur = round(numpy_time - naive_time, digits)\n",
    "    #cython_dur = round(cython_time - numpy_time, digits)\n",
    "    \n",
    "    return [naive_dur, numpy_dur]#, cython_dur]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run function to determine time to calculate determinant of functions of matrix size 1 to 11\n",
    "results_det = [rand_matrix_det(size) for size in range(1,12)]\n",
    "results_det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# isolate results from each method\n",
    "results_det_transpose = np.array(results_det).T\n",
    "python_results_det = results_det_transpose[0]\n",
    "numpy_results_det = results_det_transpose[1]\n",
    "\n",
    "# plot results\n",
    "plt.title('Matrix Determinant Execution Time by Method')\n",
    "plt.ylabel('Seconds')\n",
    "plt.xlabel('Matrix Size')\n",
    "\n",
    "# re-index values so their index corresponds to matrix size\n",
    "plt.plot(np.append(np.roll(python_results_det,1),python_results_det[-1]), 'r--', label = 'Python', alpha = 0.75)\n",
    "plt.plot(np.append(np.roll(numpy_results_det,1),numpy_results_det[-1]), 'bs', label = 'Numpy', alpha = 0.75)\n",
    "plt.legend(loc = 'upper left')\n",
    "\n",
    "# reset xticks to correspond to matrix size\n",
    "ax = plt.subplot(111)\n",
    "size = len(python_results_det) + 1\n",
    "ax.set_xlim(1, size - 1)\n",
    "dim = np.arange(1, size, 1)\n",
    "plt.xticks(dim)\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of Matrix Determinant Timing Results\n",
    "It appears that the time required to calculate the determinant of a matrix using base python increases exponentially as the size of the matrix increases, while the time required to calculate the determinant using numpy is not significantly increased as matrix size grows.\n",
    "\n",
    "For example, calculating the determinant of a random matrix of size 9, 10, and 11 using naive python requires roughly 0.8 seconds, 7.9 seconds, and 84 seconds on my machine, respectively, while the time required using numpy remains less than roughly 10^-4 seconds in each case.\n",
    "\n",
    "The reason for the increased speed of numpy is the same as discussed above for multiplying matrices, namely that numpy is a specialized package that uses arrays of homogenous data that require less memory to store and access. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IO Exercises\n",
    "\n",
    "Below is a map of various datatypes in python that you have come across and their corresponding JSON equivalents.\n",
    "\n",
    "$$Datatypes=\\left[ \\begin{array}{cc} JSON & Python3 \\\\ object & dictionary \\\\ array & list \\\\ string & string \\\\ integer\t& integer \\\\ real number & float \\\\ true & True \\\\ false & False \\\\ null & None  \\end{array} \\right]$$\n",
    "\n",
    "\n",
    "There are atleast two very important python datatypes missing in the above list. \n",
    "Can you find the same?  [list the two mising python datatypes in this markdown cell below]\n",
    "\n",
    "1. Tuple\n",
    "2. Set\n",
    "\n",
    "Now We can save the above map as a dictionary with Key-value pairs \n",
    "1. create a python dictionary named dataypes, having the above map as the Key-value pairs with Python datatypes as values and JSON equivalents as keys.\n",
    "2. Save it as a pickle called datatypes and gzip the same.\n",
    "3. Reload this pickle, and read the file contents and output the data in the following formatted way as given in this example - \"The JSON equivalent for the Python datatype Dictionary is Object\". Output similarly for the rest of the key-value pairs.\n",
    "4. Save this data as a JSON but using Python datatypes as keys and JSON equivalent as values this time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "# create datatypes dictionary\n",
    "datatypes = {\n",
    "    'JSON':'Python3',\n",
    "    'object':'dictionary',\n",
    "    'array':'list',\n",
    "    'string':'string',\n",
    "    'integer':'integer',\n",
    "    'realnumber':'float',\n",
    "    'true':'True',\n",
    "    'false':'False',\n",
    "    'null':'None'\n",
    "}\n",
    "\n",
    "# save as pickle called datatypes and gzip\n",
    "pickle.dump(datatypes,gzip.open('datatypes.pkl','wb'))\n",
    "\n",
    "# reload, read, and output data in format requested\n",
    "data = pickle.load(gzip.open('datatypes.pkl','rb'))\n",
    "for key in data.keys():\n",
    "    print \"The JSON equivalent for the Python datatype %s is %s.\" % (data[key], key)\n",
    "    \n",
    "# save as JSON with Python datatypes as keys and JSON datatypes as values\n",
    "new_dict = {}\n",
    "for key in data.keys():\n",
    "    new_dict[data[key]] = key  \n",
    "    \n",
    "json.dump(new_dict,open('new_dict.jsn','wb'))\n",
    "!cat new_dict.jsn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Data Analysis\n",
    "Pandas gives us a nice set of tools to work with columnar data (similar to R's dataframe). \n",
    "To learn how to use this it makes the most sense to use a real data set.\n",
    "For this assignment we'll use the KDD Cup 1998 dataset, which can be sourced from http://kdd.ics.uci.edu/databases/kddcup98/kddcup98.html .\n",
    "\n",
    "\n",
    "### Acquiring Data\n",
    "First we pull the README file from the dataset into this notebook via the unix \"curl\" command.  Remember you can hide/minimize output cells via the button on the left of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!curl http://kdd.ics.uci.edu/databases/kddcup98/epsilon_mirror/readme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see this README describes several files which may be of use.  In particular there are two more documentation files (DOC and DIC) we should read to get an idea of the data format.  Bring these files into the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!curl http://kdd.ics.uci.edu/databases/kddcup98/epsilon_mirror/cup98doc.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!curl http://kdd.ics.uci.edu/databases/kddcup98/epsilon_mirror/cup98dic.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we wish to download the cup98lrn.zip file and unzip it into a new subdirectory called \"data\".  \n",
    "However, since this file is pretty big we don't want to store it on github.  \n",
    "Luckily git provides the [.gitignore](http://git-scm.com/docs/gitignore) file which allows us to specify files we don't want to put into our git repository.\n",
    "\n",
    "Please do the following steps:\n",
    "\n",
    "1. Add the directory \"data\" to the .gitignore file\n",
    "2. Commit the new .gitignore file\n",
    "3. Create a new directory \"data\"\n",
    "4. Download http://kdd.ics.uci.edu/databases/kddcup98/epsilon_mirror/cup98lrn.zip into the data directory\n",
    "5. Unzip the cup98lrn.zip (we will only be using the unzipped version, so feel free to remove the zip file)\n",
    "6. Run \"git status\" to show that the data directory is not a tracked file (this indicates it is ignored)\n",
    "\n",
    "**NOTE:** These steps only need to be run once, it is advised you comment all the lines out by putting a # at the start of each line after they have run.  This will save you time in the future when you have to rerun all cells/don't want to spend a few minutes downloading the data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!echo '/data' >> .gitignore\n",
    "#!cat .gitignore\n",
    "#!git commit .gitignore -m 'added data folder to .gitignore'\n",
    "#!mkdir data\n",
    "#%cd data\n",
    "#!curl http://kdd.ics.uci.edu/databases/kddcup98/epsilon_mirror/cup98lrn.zip -o 'cup98lrn.zip'\n",
    "#!unzip 'cup98lrn.zip'\n",
    "#!rm 'cup98lrn.zip'\n",
    "#!git status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform some basic sanity checks on the data.  Using a combination of unix/basic python answer the following questions:\n",
    "\n",
    "1. How many lines are there?  \n",
    "2. Is the file character seperated or fixed width format?\n",
    "3. Is there a header?  If so how many fields are in it?\n",
    "4. Do all rows have the same number of fields as the header?\n",
    "5. Does anyhting in 1-4 disagree with the readme file or indicate erroneous data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# determine number of lines in file\n",
    "!wc -l 'data/cup98LRN.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# look at first row to see if delimited or fixed width, and to see if there is a header\n",
    "!head -1 'data/cup98LRN.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# replace commas in header with newlines and count lines to determine number of header fields\n",
    "!head -1 'data/cup98LRN.txt' | tr ',' '\\n' | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# determine count of rows of data with differing number of entries as column headers\n",
    "with open(\"data/cup98LRN.txt\") as f:\n",
    "    rows = [line.split(',') for line in f]\n",
    "    header_cols = len(rows[0])\n",
    "    mismatched_rows = []\n",
    "    for i in range(len(rows)):\n",
    "        if len(rows[i]) != header_cols:\n",
    "            mismatched_rows.append((i, rows[i]))\n",
    "            \n",
    "print len(mismatched_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give answers to questions 1-4 in this markdown cell:\n",
    "\n",
    "1. There are 95,413 lines in the file (including the header row).\n",
    "2. The file is comma separated.\n",
    "3. Yes, there is a header with 481 fields.\n",
    "4. All rows have 481 fields.\n",
    "\n",
    "Now load the data file into a pandas dataframe called \"learn\".  To save some time, we've loaded the data dictionary into col_types.  \n",
    "\n",
    "Finally split learn into two data frames, learn_y: the targets (two columns described in the documentation) and learn_x: the predictors (everything but the targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict_file = open(\"dict.dat\")\n",
    "col_types = [ (x.split(\"\\t\")[0], x.strip().split(\"\\t\")[1]) for x in dict_file.readlines() ]\n",
    "#col_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create data dictionary from 'col_types'\n",
    "data_dict = {}\n",
    "for col in col_types:\n",
    "    data_dict[col[0]] = col[1]\n",
    "    if col[1] == 'Num':\n",
    "        data_dict[col[0]] = 'float64'\n",
    "    else:\n",
    "        data_dict[col[0]] = 'object'\n",
    "\n",
    "#data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data into learn dataframe\n",
    "learn = pd.read_csv('data/cup98LRN.txt', dtype = data_dict)\n",
    "\n",
    "# Split TARGET_B and TARGET_D into learn_y dataframe\n",
    "learn_y = learn[['TARGET_B', 'TARGET_D']]\n",
    "\n",
    "# Remove TARGET_B and TARGET_D from learn_x dataframe\n",
    "learn_x = learn.drop(['TARGET_B', 'TARGET_D'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learn_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learn_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing Data\n",
    "Now that we have loaded data into the learn table, we wish to to summarize the data.  \n",
    "Write a function called summary which takes a pandas data frame and prints a summary of each column containing the following:\n",
    "\n",
    "If the column is numeric:\n",
    "\n",
    "1. Mean\n",
    "2. Standard Deviation\n",
    "3. Min/Max\n",
    "4. Number of missing values (NaN, Inf, NA)\n",
    "\n",
    "If the column is non numeric:\n",
    "\n",
    "1. Number of distinct values\n",
    "2. Number of missing values (NaN, INF, NA, blank/all spaces)\n",
    "3. The frequency of the 3 most common values and 3 least common values\n",
    "\n",
    "Format the output to be human readable.\n",
    "\n",
    "For example:\n",
    "> Field_1  \n",
    "> mean: 50  \n",
    "> std_dev: 25  \n",
    "> min: 0  \n",
    "> max: 100  \n",
    "> missing: 5\n",
    ">  \n",
    "> Field_2  \n",
    "> distinct_values: 100  \n",
    "> missing: 10  \n",
    ">  \n",
    "> 3 most common:  \n",
    ">   the: 1000  \n",
    ">   cat: 950  \n",
    ">   meows: 900  \n",
    ">  \n",
    "> 3 least common:  \n",
    ">   dogs: 5  \n",
    ">   lizards: 4  \n",
    ">   eggs: 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to summarize dataframe of format similar to 'learn_x'\n",
    "def summary(df):\n",
    "    cols = list(df.columns.values)\n",
    "    df['count'] = 1 # adding count column for convenience\n",
    "    \n",
    "    # replace blanks and empty spaces with nan\n",
    "    df.replace('', np.nan, inplace = True)\n",
    "    df.replace(' ', np.nan, inplace = True)\n",
    "    \n",
    "    for col in cols:\n",
    "        data = df[col]\n",
    "        missing = data.isnull().sum()\n",
    "        \n",
    "        print col\n",
    "        if data_dict[col] == 'float64':\n",
    "            mean = data.mean()\n",
    "            sd = data.std()\n",
    "            minimum = data.min()\n",
    "            maximum = data.max()\n",
    "                \n",
    "            print 'mean:     %.2f' % (mean)\n",
    "            print 'std dev:  %.2f' % (sd)\n",
    "            print 'min:      %.2f' % (minimum)\n",
    "            print 'max:      %.2f' % (maximum)\n",
    "            print 'missing:  %d\\n' % (missing)\n",
    "        elif data_dict[col] == 'object':\n",
    "            distinct = len(data.dropna().unique())\n",
    "            \n",
    "            print 'distinct: %d' % (distinct)\n",
    "            print 'missing:  %d\\n' % (missing)\n",
    "\n",
    "            tmp_df = df.groupby(col).sum()\n",
    "            tmp_df = tmp_df.sort_values('count', ascending = 0)\n",
    "            tmp_df = tmp_df.reset_index()\n",
    "            \n",
    "            print '3 most common:'\n",
    "            most1 = tmp_df[col][0]\n",
    "            most1count = tmp_df['count'][0]\n",
    "            print '%s: %d' % (most1, most1count)\n",
    "\n",
    "            if distinct >= 2:\n",
    "                most2 = tmp_df[col][1]\n",
    "                most2count = tmp_df['count'][1]\n",
    "                print '%s: %d' % (most2, most2count)\n",
    "            else:\n",
    "                print 'No second most common element'\n",
    "            \n",
    "            if distinct >= 3:\n",
    "                most3 = tmp_df[col][2]\n",
    "                most3count = tmp_df['count'][2]\n",
    "                print '%s: %d\\n' % (most3, most3count)\n",
    "            else:\n",
    "                print 'No third most common element\\n'\n",
    "            \n",
    "            tmp_df2 = df.groupby(col).sum()\n",
    "            tmp_df2 = tmp_df2.sort_values('count', ascending = 1)\n",
    "            tmp_df2 = tmp_df2.reset_index()\n",
    "\n",
    "            print '3 least common:'\n",
    "            least1 = tmp_df2[col][0]\n",
    "            least1count = tmp_df2['count'][0]\n",
    "            print '%s: %d' % (least1, least1count)\n",
    "\n",
    "            if distinct >= 2:\n",
    "                least2 = tmp_df2[col][1]\n",
    "                least2count = tmp_df2['count'][1]\n",
    "                print '%s: %d' % (least2, least2count)\n",
    "            else:\n",
    "                print 'No second least common element'\n",
    "            \n",
    "            if distinct >= 3:\n",
    "                least3 = tmp_df2[col][2]\n",
    "                least3count = tmp_df2['count'][2]\n",
    "                print '%s: %d\\n' % (least3, least3count)\n",
    "            else:\n",
    "                print 'No third least common element\\n'\n",
    "        else:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "# summarize 'learn_x' dataframe\n",
    "summary(learn_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " ### Pandas analysis on Calit2 data \n",
    "\n",
    "Import data from http://archive.ics.uci.edu/ml/machine-learning-databases/event-detection/CalIt2.data using curl\n",
    "\n",
    "This data comes from the main door of the CalIt2 building at UCI. Observations come from 2 data streams (people flow in and out of the building), over 15 weeks, 48 time slices per day (half hour count aggregates).\n",
    "\n",
    "Attribute Information:\n",
    "1. Flow ID: 7 is out flow, 9 is in flow\n",
    "2. Date: MM/DD/YY\n",
    "3. Time: HH:MM:SS\n",
    "4. Count: Number of counts reported for the previous half hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!curl http://archive.ics.uci.edu/ml/machine-learning-databases/event-detection/CalIt2.data > CalIt2.data\n",
    "df = pd.read_csv('CalIt2.data', names = ['Flow_ID','Date','Time','Count'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Selecting Data ####\n",
    "1. Select all data for the date July 24 2005 having flow id=7. Also output the row count of results \n",
    "2. Select all rows whose count is greater than 5. Sort the result on count in descending order and output the top 10 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# all rows for 'Date' July 24 2005 having 'Flow_ID' 7\n",
    "subset_df = df.loc[(df['Flow_ID'] == 7) & (df['Date'] == '07/24/05')]\n",
    "\n",
    "# print first few rows of data\n",
    "print subset_df.head()\n",
    "\n",
    "# output row count\n",
    "print 'row count = %d' % (subset_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# select rows whose count is greater than 5 and print top 10 rows by count\n",
    "subset_df2 = df[df['Count'] > 5].sort_values('Count', ascending = 0)\n",
    "#subset_df2.sort_values('Count', ascending = 0)\n",
    "subset_df2 = subset_df2.iloc[0:10]\n",
    "subset_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply function ####\n",
    "1. For the 10 rows outputted above, use Pandas Apply function to subtract lowest value of the 10 from all of them and then output the average value of the resulting counts\n",
    "2. On the entire data, use apply function to sum all counts with flow_id=9 and date is 07/24/05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use apply to subtract lowest count from top 10 and output average of resulting counts\n",
    "lowest_count = subset_df2['Count'].values.min()\n",
    "print subset_df2['Count'].apply(lambda(x): x - lowest_count).values.mean()\n",
    "\n",
    "# use apply to sum all counts with Flow_ID = 9 and Date = 07/24/05\n",
    "print df.apply(lambda(x): x.loc[(df['Flow_ID'] == 9) & (df['Date'] == '07/24/05')])['Count'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing an Selecting ####\n",
    "Exlain the following\n",
    "\n",
    "1. loc:  subsets a dataframe based on index label; most suitable when a dataframe has meaningful and clearly labeled non-numerical indices (such as months in order)\n",
    "\n",
    "2. iloc:  subsets a dataframe based on numerical index position; suitable use case would be returning the top N rows of a dataframe after sorting\n",
    "\n",
    "3. ix:  subsets a dataframe based on index label or position; most general and flexible but can be confusing and produce unexpected results; use case would be subsetting a dataframe with mixed positional and label based indices\n",
    "\n",
    "4. at:  similar to loc, but faster, and can only be used to access a single element from a dataframe; use case is the same as loc, but if you only need to access a single element from the dataframe\n",
    "\n",
    "5. iat:  similar to iloc, but faster, and can only be used to access a single element from a dataframe; use case is the same as iloc, but if you only need to access a single element from the dataframe\n",
    "\n",
    "Highlight the differences by providing usecases where one is more useful than the other\n",
    "\n",
    "Write a function to take two dates as input and return all flow ids and counts in that date range having both the dates inclusive. You can use pandas to_datetime function to convert the date to pandas datetime format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# function to return all flow ids and counts falling within specified date range\n",
    "def dateRange(start, end):\n",
    "    return_df = df\n",
    "    return_df['Date'] =  pd.to_datetime(return_df['Date'], format='%m/%d/%y')\n",
    "    return_df = return_df.sort_values('Date', ascending = 1)\n",
    "    return_df = return_df.set_index('Date')\n",
    "    return_df = return_df.loc[start:end]\n",
    "    return_df.reset_index(level=0, inplace=True)\n",
    "    return_df = return_df.sort_values(['Date','Time'], ascending = [1, 1])\n",
    "    return_df['Date'] = return_df['Date'].dt.strftime('%m/%d/%y')\n",
    "    return_df = return_df[['Flow_ID','Date','Time','Count']]\n",
    "    return return_df\n",
    "\n",
    "# test dateRange function\n",
    "test_df = dateRange('08/01/05','08/31/05')\n",
    "print test_df.head()\n",
    "print '\\n'\n",
    "print test_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping ####\n",
    "1. Select data in the month of August 2005 having flow id=7\n",
    "2. Group the data based on date and get the max count per date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use dateRange function written above to select data for August 2005\n",
    "subset_august = dateRange('08/01/05','08/31/05')\n",
    "\n",
    "# subset August 2005 data by flow id = 7\n",
    "subset_august = subset_august[subset_august['Flow_ID'] == 7]\n",
    "\n",
    "subset_august.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subset_august.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# select date in August 2005 with flow id 7\n",
    "#subset_df = df[df.Date.str.startswith('08')]\n",
    "#subset_df = subset_df[subset_df.Date.str.endswith('05')]\n",
    "subset_df = dateRange('')\n",
    "subset_df = subset_df[subset_df['Flow_ID'] == 7]\n",
    "\n",
    "subset_df\n",
    "'''\n",
    "# group by date and get max count by date\n",
    "print subset_august.groupby('Date')['Count'].max()\n",
    "\n",
    "# in case the intention was to group by date and get total count by date\n",
    "#print subset_august.groupby('Date')['Count'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacking, Unstacking ####\n",
    "1. Stack the data with count and flow_id as indexes\n",
    "2. Use reset_index to reset the stacked hierarchy by 1 level. The index then will just be the counts\n",
    "3. Unstack the data to get back original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# stacking then applying 'reset_index'\n",
    "stacked_df = df.set_index(['Flow_ID', 'Count'], append = True)\n",
    "stacked_df = stacked_df.stack()\n",
    "stacked_df = stacked_df.reset_index()\n",
    "#stacked_df = stacked_df.unstack()\n",
    "print stacked_df.head()\n",
    "\n",
    "# print newline for readability\n",
    "print '\\n'\n",
    "\n",
    "# stacking and unstacking, then applying 'reset_index'\n",
    "stacked_df = df.set_index(['Flow_ID', 'Count'], append = True)\n",
    "stacked_df = stacked_df.stack()\n",
    "stacked_df = stacked_df.unstack()\n",
    "stacked_df = stacked_df.reset_index(['Flow_ID','Count'])\n",
    "stacked_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas and Matplotlib\n",
    "\n",
    "Plot a histogram of date vs total counts for flow_id=7 and flow_id=9 for the month of July 2005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use dateRange function to create dataframe with only data from July 2005\n",
    "july_df = dateRange('07/01/05','07/31/05')\n",
    "\n",
    "# reformat dataframe to get flow ids in separate columns\n",
    "hist_df = july_df.groupby(['Date','Flow_ID'])['Count'].sum().unstack('Flow_ID')\n",
    "\n",
    "# extract data by flow_id\n",
    "f7 = hist_df[7]\n",
    "f9 = hist_df[9]\n",
    "\n",
    "# determine dates and set x_index values\n",
    "dates = july_df['Date'].unique()\n",
    "x_index = np.arange(len(dates))\n",
    "\n",
    "# plot Flow_ID = 7\n",
    "plt.bar(x_index, f7, align='center', color='blue', alpha=0.5)\n",
    "plt.xticks(x_index, dates, rotation = 90)\n",
    "plt.title('Daily July Counts for Flow ID = 7')\n",
    "plt.xlabel('Date in July 2005') \n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# plot Flow_ID = 9\n",
    "plt.bar(x_index, f9, align='center', color='green', alpha=0.5)\n",
    "plt.xticks(x_index, dates, rotation = 90)\n",
    "plt.title('Daily July Counts for Flow ID = 9')\n",
    "plt.xlabel('Date in July 2005') \n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python2]",
   "language": "python",
   "name": "conda-env-python2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
