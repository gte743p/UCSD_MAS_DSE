{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc=SparkContext(master=\"local[4]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "from string import split,strip\n",
    "\n",
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "\n",
    "from pyspark.mllib.util import MLUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higgs data set\n",
    "* **URL:** http://archive.ics.uci.edu/ml/datasets/HIGGS#  \n",
    "* **Abstract:** This is a classification problem to distinguish between a signal process which produces Higgs bosons and a background process which does not.\n",
    "\n",
    "**Data Set Information:**  \n",
    "The data has been produced using Monte Carlo simulations. The first 21 features (columns 2-22) are kinematic properties measured by the particle detectors in the accelerator. The last seven features are functions of the first 21 features; these are high-level features derived by physicists to help discriminate between the two classes. There is an interest in using deep learning methods to obviate the need for physicists to manually develop such features. Benchmark results using Bayesian Decision Trees from a standard physics package and 5-layer neural networks are presented in the original paper. The last 500,000 examples are used as a test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 ['lepton pT', 'lepton eta', 'lepton phi', 'missing energy magnitude', 'missing energy phi', 'jet 1 pt', 'jet 1 eta', 'jet 1 phi', 'jet 1 b-tag', 'jet 2 pt', 'jet 2 eta', 'jet 2 phi', 'jet 2 b-tag', 'jet 3 pt', 'jet 3 eta', 'jet 3 phi', 'jet 3 b-tag', 'jet 4 pt', 'jet 4 eta', 'jet 4 phi', 'jet 4 b-tag', 'm_jj', 'm_jjj', 'm_lv', 'm_jlv', 'm_bb', 'm_wbb', 'm_wwbb']\n"
     ]
    }
   ],
   "source": [
    "#define feature names\n",
    "feature_text='lepton pT, lepton eta, lepton phi, missing energy magnitude, missing energy phi, jet 1 pt, jet 1 eta, jet 1 phi, jet 1 b-tag, jet 2 pt, jet 2 eta, jet 2 phi, jet 2 b-tag, jet 3 pt, jet 3 eta, jet 3 phi, jet 3 b-tag, jet 4 pt, jet 4 eta, jet 4 phi, jet 4 b-tag, m_jj, m_jjj, m_lv, m_jlv, m_bb, m_wbb, m_wwbb'\n",
    "features=[strip(a) for a in split(feature_text,',')]\n",
    "print len(features),features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/joshwilson/Documents/DSE/CSE255-DSE230/Classes/HW-7/higgs\n",
      "total 15694336\n",
      "-rw-r--r--  1 joshwilson  staff  8035497980 May 31 18:37 HIGGS.csv\n",
      "/Users/joshwilson/Documents/DSE/CSE255-DSE230/Classes/HW-7\n"
     ]
    }
   ],
   "source": [
    "# create a directory called higgs, download and decompress HIGGS.csv.gz into it\n",
    "\n",
    "from os.path import exists\n",
    "if not exists('higgs'):\n",
    "    print \"creating directory higgs\"\n",
    "    !mkdir higgs\n",
    "%cd higgs\n",
    "if not exists('HIGGS.csv'):\n",
    "    if not exists('HIGGS.csv.gz'):\n",
    "        print 'downloading HIGGS.csv.gz'\n",
    "        !curl -O http://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz\n",
    "    print 'decompressing HIGGS.csv.gz --- May take 5-10 minutes'\n",
    "    !gunzip -f HIGGS.csv.gz\n",
    "!ls -l\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As done in previous notebook, create RDDs from raw data and build Gradient boosting and Random forests models. Consider doing 1% sampling since the dataset is too big for your local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'1.000000000000000000e+00,8.692932128906250000e-01,-6.350818276405334473e-01,2.256902605295181274e-01,3.274700641632080078e-01,-6.899932026863098145e-01,7.542022466659545898e-01,-2.485731393098831177e-01,-1.092063903808593750e+00,0.000000000000000000e+00,1.374992132186889648e+00,-6.536741852760314941e-01,9.303491115570068359e-01,1.107436060905456543e+00,1.138904333114624023e+00,-1.578198313713073730e+00,-1.046985387802124023e+00,0.000000000000000000e+00,6.579295396804809570e-01,-1.045456994324922562e-02,-4.576716944575309753e-02,3.101961374282836914e+00,1.353760004043579102e+00,9.795631170272827148e-01,9.780761599540710449e-01,9.200048446655273438e-01,7.216574549674987793e-01,9.887509346008300781e-01,8.766783475875854492e-01'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the file into an RDD\n",
    "# If doing this on a real cluster, you need the file to be available on all nodes, ideally in HDFS.\n",
    "path='higgs/HIGGS.csv'\n",
    "inputRDD=sc.textFile(path)\n",
    "inputRDD.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(1.0, [0.869293212891,-0.635081827641,0.22569026053,0.327470064163,-0.689993202686,0.754202246666,-0.24857313931,-1.09206390381,0.0,1.37499213219,-0.653674185276,0.930349111557,1.10743606091,1.13890433311,-1.57819831371,-1.0469853878,0.0,0.65792953968,-0.0104545699432,-0.0457671694458,3.10196137428,1.35376000404,0.979563117027,0.978076159954,0.920004844666,0.721657454967,0.988750934601,0.876678347588])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the text RDD into an RDD of LabeledPoints\n",
    "Data=inputRDD.map(lambda line: [float(strip(x)) for x in line.split(',')])\\\n",
    "     .map(lambda line: LabeledPoint(line[:1][0],line[1:]))\n",
    "Data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sizes: Data1=110487, trainingData=77477, testData=33010\n"
     ]
    }
   ],
   "source": [
    "# full dataset for cluster\n",
    "#Data1=Data.sample(False,0.1).cache()\n",
    "#(trainingData,testData)=Data1.randomSplit([0.7,0.3],seed=255)\n",
    "\n",
    "# subset of dataset for local testing\n",
    "Data1=Data.sample(False,0.01).cache()\n",
    "(trainingData,testData)=Data1.randomSplit([0.7,0.3],seed=255)\n",
    "\n",
    "print 'Sizes: Data1=%d, trainingData=%d, testData=%d'%(Data1.count(),trainingData.cache().count(),testData.cache().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0.2 20 {'test': 0.2979400181763102, 'train': 0.12244924300114873} 620 seconds\n"
     ]
    }
   ],
   "source": [
    "# gradient boosted tree\n",
    "from time import time\n",
    "errors={}\n",
    "for depth in [10]:\n",
    "    for lr in [0.2]:\n",
    "        for numiter in [20]:\n",
    "            start=time()\n",
    "            model=GradientBoostedTrees.trainClassifier(trainingData, categoricalFeaturesInfo={}, \n",
    "                                                       numIterations=numiter, maxDepth=depth, \n",
    "                                                       learningRate=lr)\n",
    "            #print model.toDebugString()\n",
    "            errors[depth]={}\n",
    "            dataSets={'train':trainingData,'test':testData}\n",
    "            for name in dataSets.keys():  # Calculate errors on train and test sets\n",
    "                data=dataSets[name]\n",
    "                Predicted=model.predict(data.map(lambda x: x.features))\n",
    "                LabelsAndPredictions=data.map(lambda LP: LP.label).zip(Predicted)\n",
    "                Err = LabelsAndPredictions.filter(lambda (v,p):v != p).count()/float(data.count())\n",
    "                errors[depth][name]=Err\n",
    "            print depth,lr,numiter,errors[depth],int(time()-start),'seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 {'test': 0.30187821872159953, 'train': 0.25869612917381934} 64 seconds\n"
     ]
    }
   ],
   "source": [
    "# random forest\n",
    "from time import time\n",
    "for depth in [10]:\n",
    "    for ntrees in [15]:\n",
    "        for imp in ['gini']:\n",
    "            start=time()\n",
    "            model = RandomForest.trainClassifier(trainingData, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                                 numTrees=ntrees, featureSubsetStrategy='auto',\n",
    "                                                 impurity=imp, maxDepth=depth, \n",
    "                                                 maxBins=32, seed=11)\n",
    "            #print model.toDebugString()\n",
    "            errors[depth]={}\n",
    "            dataSets={'train':trainingData,'test':testData}\n",
    "            for name in dataSets.keys():  # Calculate errors on train and test sets\n",
    "                data=dataSets[name]\n",
    "                Predicted=model.predict(data.map(lambda x: x.features))\n",
    "                LabelsAndPredictions=data.map(lambda LP: LP.label).zip(Predicted)\n",
    "                Err = LabelsAndPredictions.filter(lambda (v,p):v != p).count()/float(data.count())\n",
    "                errors[depth][name]=Err\n",
    "            print depth,errors[depth],int(time()-start),'seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
