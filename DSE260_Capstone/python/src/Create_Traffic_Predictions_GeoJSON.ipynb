{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Traffic Predictions GeoJSON\n",
    "\n",
    "Notebook assumes modeling pipeline has been run:\n",
    "- save_dir as defined in args file has been populated with test_predictions.csv\n",
    "- database has been populated with segments_selected table and time_N table, where N = time_resolution as defined in args file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. get predictions data\n",
    "2. create geojson and write to file\n",
    "3. create timestamp lookup and bounding box and write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "ub6Tid1fopr-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import joblib\n",
    "import ast\n",
    "import json\n",
    "import pandas as pd\n",
    "import psycopg2 as pg\n",
    "import datetime as dt\n",
    "from AWS import AWS\n",
    "from Utility import Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set environment\n",
    "aws = None\n",
    "s3_bucket_name = 'dse-cohort3-group3'\n",
    "s3_dat_dir = 'PreprocessedWazeData'\n",
    "\n",
    "# get args file\n",
    "sampling_args_file = '../conf/pipeline_args.txt'\n",
    "fr = open(sampling_args_file, 'r')\n",
    "fa = fr.read()\n",
    "file_args = ast.literal_eval(fa)\n",
    "\n",
    "# assume save_dir already exists\n",
    "save_dir = file_args['save_dir']\n",
    "\n",
    "# assume connection file is always present\n",
    "conn_str_file = file_args['conn_str_file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create subdirectory for viz files - assumes save_dir already exists\n",
    "viz_dir = '{}/viz_files'.format(save_dir)\n",
    "\n",
    "# delete and remake viz_dir if exists\n",
    "if os.path.isdir(viz_dir):\n",
    "    shutil.rmtree(viz_dir)\n",
    "\n",
    "os.mkdir(viz_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cKQxFQOfopsE"
   },
   "source": [
    "## 1. get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create utility object and connect to database\n",
    "util = Utility(file_args)\n",
    "\n",
    "if util.isAWS():\n",
    "    aws = AWS(s3_bucket_name, s3_dat_dir)\n",
    "\n",
    "pg_conn_str = open(conn_str_file, 'r').read()\n",
    "\n",
    "conn = pg.connect(pg_conn_str) \n",
    "util.conn = conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "sYVUN76jopsG"
   },
   "outputs": [],
   "source": [
    "def mile_to_meter(mile):\n",
    "    meter = int(round(mile*1609.344))\n",
    "    return str(meter)\n",
    "\n",
    "def bounding_box(nw, se):\n",
    "    nw = nw.replace(\" N\",\"\").replace(\" W\", \"\")\n",
    "    nw = nw.split(', ')\n",
    "    se = se.replace(\" N\",\"\").replace(\" W\", \"\")\n",
    "    se = se.split(', ')\n",
    "    \n",
    "    return \"geom @ ST_MakeEnvelope (-{}, {}, -{}, {}) and ST_Length(geom) > 0\".format(nw[1],nw[0],se[1],se[0])\n",
    "\n",
    "def sql_radius(google_lat_lon, mile):\n",
    "    lat_lon = google_lat_lon.replace(\" N\",\"\").replace(\" W\", \"\")\n",
    "    lat_lon = lat_lon.split(', ')\n",
    "    return \"ST_DWithin(geom, ST_MakePoint(\" + \"-\"+ lat_lon[1] +\",\" + lat_lon[0] + \")::geography,\" + mile_to_meter(mile) +')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "id": "XocnlCOxopsI",
    "outputId": "a8ad7236-9b3d-40a5-9667-af7765554416"
   },
   "outputs": [],
   "source": [
    "# get segments\n",
    "select_attributes = \"segment_id, street, road_type, ST_AsGeoJSON(geom) AS geometry\"\n",
    "where_conditions = bounding_box(file_args['segment_queries']['bounding_box']['input_nw_corner'], \n",
    "                                file_args['segment_queries']['bounding_box']['input_se_corner'])\n",
    "segments_sql = \"SELECT \" + select_attributes + \" FROM segments_selected WHERE \" + where_conditions\n",
    "\n",
    "segments_df = pd.read_sql(segments_sql, con=conn)\n",
    "print(str(len(segments_df)) + \" rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {},
      {}
     ]
    },
    "colab_type": "code",
    "id": "zO7wuXu4opsR",
    "outputId": "aef47c39-9daa-4e0c-8c97-f69d98563b11"
   },
   "outputs": [],
   "source": [
    "# get time windows for range of test dates\n",
    "times_sql = '''\n",
    "select date, time, day_of_week\n",
    "from time_''' + str(file_args['time_resolution']) + '''\n",
    "where date >= \\'''' + file_args['test_date_start'] + '''\\' and date <= \\'''' + file_args['test_date_end'] + '''\\';'''\n",
    "\n",
    "time_df = pd.read_sql(times_sql, con=conn)\n",
    "time_df.sort_values(by=['date','time'], inplace=True)\n",
    "print(str(len(time_df))+\" rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {},
      {}
     ]
    },
    "colab_type": "code",
    "id": "VUkBzqIbopsV",
    "outputId": "46b49470-23b2-4ef8-f0cc-d92cd2eaa972",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cartesian Product of segments and times\n",
    "time_df['tmp'] = 1\n",
    "segments_df['tmp'] = 1\n",
    "segments_time_df = pd.merge(time_df, segments_df, how='outer', on=['tmp'])\n",
    "print(str(len(segments_time_df))+\" rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # get actuals and predictions from database\n",
    "# preds_df = pd.read_sql('select * from test_predictions;', con=conn)\n",
    "# print(str(len(preds_df))+\" rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get actuals and predictions from csv\n",
    "fn = os.path.join(save_dir, 'test_predictions.csv')\n",
    "preds_df = pd.read_csv(fn)\n",
    "preds_df['date'] = pd.to_datetime(preds_df['date'],format='%Y-%m-%d').dt.date\n",
    "preds_df['time'] = pd.to_datetime(preds_df['time'],format='%H:%M:%S').dt.time\n",
    "print(str(len(preds_df))+\" rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "id": "VDSDx6I0opsv",
    "outputId": "134eecb8-3cbc-4a6a-fb04-ec4acacfd6da"
   },
   "outputs": [],
   "source": [
    "#Left join preds_df onto df with cartesian product of all time slices and segments of interest\n",
    "level_cols = [c for c in preds_df.columns if c.startswith('level')]\n",
    "geojson_df = pd.merge(segments_time_df, preds_df[['date','time','segment_id']+level_cols], \n",
    "                      how='left', on=['segment_id','date','time'])\n",
    "print(str(len(geojson_df))+\" rows\")\n",
    "\n",
    "#Replace nulls with 0 for assumption of no congestion\n",
    "geojson_df.update(geojson_df[level_cols].fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#level_cols = ['level_max','level_max_preds_cluster_logistic_regression','level_max_preds_full_knn',\n",
    "#              'level_max_preds_cluster_ensemble','level_max_preds_avg_baseline']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  create geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create day of week dictionary - might want to make this actual date range?\n",
    "dow_dict = {\n",
    "    '1':'Sunday',\n",
    "    '2':'Monday',\n",
    "    '3':'Tuesday',\n",
    "    '4':'Wednesday',\n",
    "    '5':'Thursday',\n",
    "    '6':'Friday',\n",
    "    '7':'Saturday'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to extract segment data by day of week\n",
    "def extract_segment_data(seg):\n",
    "    seg_data_dict = {}\n",
    "    for dow,dow_name in dow_dict.iteritems():\n",
    "        # extract data for given segment and dow\n",
    "        seg_dow_df = geojson_df.loc[(geojson_df['segment_id']==seg) & (geojson_df['day_of_week']==dow),level_cols]\n",
    "        seg_data_dict[dow_name] = seg_dow_df.to_dict(orient='list')\n",
    "    return seg_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create dict to fill in with geojson\n",
    "geojson_dict = {\n",
    "    'type': 'FeatureCollection',\n",
    "    'features': []\n",
    "}\n",
    "\n",
    "# fill geojson\n",
    "for seg in geojson_df['segment_id'].unique():\n",
    "    features_dict = {\n",
    "        'type': 'Feature',\n",
    "        'geometry': ast.literal_eval(segments_df[segments_df['segment_id']==seg]['geometry'].values[0]),\n",
    "        'properties': {\n",
    "            'segment_id': seg,\n",
    "            'street': segments_df[segments_df['segment_id']==seg]['street'].values[0],\n",
    "            'road_type': segments_df[segments_df['segment_id']==seg]['road_type'].values[0],\n",
    "            'data': extract_segment_data(seg)\n",
    "        }\n",
    "    }\n",
    "    geojson_dict['features'].append(features_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dump geojson to file\n",
    "fn = os.path.join(viz_dir, 'segment_preds.geojson')\n",
    "with open(fn, 'w') as f:\n",
    "    json.dump(geojson_dict, f, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write geojson as js file to js folder\n",
    "geojson_prefix = 'var segment_preds_geojson = '\n",
    "fn = os.path.join(viz_dir, 'segment_preds_geojson.js')\n",
    "with open(fn, 'w') as f:\n",
    "    f.write(geojson_prefix + str(geojson_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. create timestamp lookup and bounding box variable files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to format string of type 'hh:mm:ss' to 'hh:mm a.m.'\n",
    "def ts_conversion(ts_str):\n",
    "    ts_hour = ts_str[:2]\n",
    "    ts_min = ts_str[3:5]\n",
    "    am_pm = 'a.m.' if int(ts_hour)<12 else 'p.m.'\n",
    "    ts_hour = str((int(ts_hour)+12) % 12)\n",
    "    ts_hour = '12' if ts_hour=='0' else ts_hour\n",
    "    ts = '{}:{} {}'.format(ts_hour, ts_min, am_pm)\n",
    "    return ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create df with all possible timestamps to ensure none are left out\n",
    "time_interval = file_args['time_resolution']\n",
    "ts_df = pd.DataFrame.from_dict({'time_interval':[dt.time(h, m, 0) for h in range(0,24) for m in range(0,60,time_interval)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert time_interval to string and write to dict\n",
    "ts_df['time_interval'] = map(ts_conversion, ts_df['time_interval'].astype(str))\n",
    "timestamp_dict = ts_df.to_dict()['time_interval']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write timestamp_lookup as js file to js folder\n",
    "ts_prefix = 'var timestamp_lookup = '\n",
    "fn = os.path.join(viz_dir, 'timestamp_lookup.js')\n",
    "with open(fn, 'w') as f:\n",
    "    f.write(ts_prefix + str(timestamp_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write bounding box bounds as js file to js folder\n",
    "bb_prefix = 'var bounds = '\n",
    "\n",
    "input_nw_corner = file_args['segment_queries']['bounding_box']['input_nw_corner']\n",
    "nw = input_nw_corner.replace(\" N\",\"\").replace(\" W\", \"\")\n",
    "nw = nw.split(', ')\n",
    "nw[1] = '-'+nw[1] # add negative\n",
    "nw = map(float, nw)\n",
    "\n",
    "input_se_corner = file_args['segment_queries']['bounding_box']['input_se_corner']\n",
    "se = input_se_corner.replace(\" N\",\"\").replace(\" W\", \"\")\n",
    "se = se.split(', ')\n",
    "se[1] = '-'+se[1] # add negative\n",
    "se = map(float, se)\n",
    "\n",
    "bounds = [[nw[0],nw[1]],[se[0],se[1]]]\n",
    "\n",
    "fn = os.path.join(viz_dir, 'boundingbox.js')\n",
    "with open(fn, 'w') as f:\n",
    "    f.write(bb_prefix + str(bounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "Phase 1 - Averages.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
