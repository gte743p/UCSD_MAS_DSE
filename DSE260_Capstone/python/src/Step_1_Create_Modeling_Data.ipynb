{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Create Modeling Data\n",
    "1. Get data from database\n",
    "2. Process data\n",
    "3. Add features\n",
    "4. Train/Test split - random split and by date\n",
    "5. Cluster - perform all clustering methods on train data with random split and by date\n",
    "6. Concatenate train and test data\n",
    "7. Write processed data to database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2 as pg\n",
    "import datetime as dt\n",
    "from sklearn import preprocessing\n",
    "from collections import OrderedDict\n",
    "from pprint import pprint\n",
    "import cPickle as pickle\n",
    "import gc\n",
    "import socket\n",
    "import boto3\n",
    "from boto.utils import get_instance_metadata\n",
    "import ast\n",
    "from Segments import Segments\n",
    "from Times import Times\n",
    "from Cluster import Cluster\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import shutil\n",
    "import joblib\n",
    "import string\n",
    "from AWS import AWS\n",
    "from Utility import Utility\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set environment\n",
    "aws = None\n",
    "s3_bucket_name = 'dse-cohort3-group3'\n",
    "s3_dat_dir = 'PreprocessedWazeData'\n",
    "\n",
    "# assume connection file is always present\n",
    "conn_str_file = '../conf/db_conn_str.txt'\n",
    "sqlalchemy_conn_str_file = '../conf/sqlalchemy_conn_str.txt'\n",
    "sampling_args_file = '../conf/pipeline_args.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fr = open(sampling_args_file, 'r')\n",
    "fa = fr.read()\n",
    "file_args = ast.literal_eval(fa)\n",
    "file_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = file_args['save_dir']\n",
    "save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if os.path.isdir('./{}'.format(save_dir)):\n",
    "    now = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    os.rename('./{}'.format(save_dir), './{}_{}'.format(save_dir, now))\n",
    "\n",
    "os.mkdir(save_dir)\n",
    "shutil.copy(sampling_args_file, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. get data from db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_data_start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create AWS object and helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "util = Utility(file_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if util.isAWS():\n",
    "    aws = AWS(s3_bucket_name, s3_dat_dir)\n",
    "\n",
    "pg_conn_str = open(conn_str_file, 'r').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### connect to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn = pg.connect(pg_conn_str) \n",
    "util.conn = conn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create Segments object and run queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#samping_options = ['radius', 'sample', 'bounding_box', 'street', \n",
    "# 'road_type', 'ignore', 'cum_seg_pct']\n",
    "#queries = [ 'sample', 'road_type', 'cum_seg_pct']\n",
    "\n",
    "segments = Segments(conn, file_args['segment_queries_to_run'], file_args)\n",
    "segments.run_queries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create Times object and run queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#samping_options = ['time_window', 'day_of_week', 'exclude_dates', 'cum_ts_pct']\n",
    "#queries = ['time_window', 'cum_ts_pct']\n",
    "\n",
    "times = Times(conn, file_args['time_queries_to_run'], file_args)\n",
    "times.run_queries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create elbow charts for top N% of segments and times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cum_seg_pct' in file_args['segment_queries_to_run']:\n",
    "    seg_cum_pct_df = pd.read_sql('select distinct cum_seg_pct, cum_pos_pct from seg_cum_pct order by cum_seg_pct', con=conn)\n",
    "\n",
    "    # plot cum pct for number of positive instances and number of segments\n",
    "    x = seg_cum_pct_df.cum_pos_pct.values\n",
    "    y = seg_cum_pct_df.cum_seg_pct.values\n",
    "    plt.plot(x,y)\n",
    "    plt.xlabel('cum positive pct')\n",
    "    plt.ylabel('cum segment pct')\n",
    "    plt.title('pct of segments required to capture pct of incidents')\n",
    "    plt.grid()\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cum_ts_pct' in file_args['time_queries_to_run']:\n",
    "    ts_cum_pct_df = pd.read_sql('select distinct cum_ts_pct, cum_pos_pct from ts_cum_pct order by cum_ts_pct', con=conn)\n",
    "\n",
    "    # plot cum pct for number of positive instances and number of timestamps\n",
    "    x = ts_cum_pct_df.cum_pos_pct.values\n",
    "    y = ts_cum_pct_df.cum_ts_pct.values\n",
    "    plt.plot(x,y)\n",
    "    plt.xlabel('cum positive pct')\n",
    "    plt.ylabel('cum time pct')\n",
    "    plt.title('pct of timestamps required to capture pct of incidents')\n",
    "    plt.grid()\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--- getting data took {0:.1f} seconds ---'.format(time.time() - get_data_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_processing_start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cartesian product of segments and times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create segments_df\n",
    "segments_select = \"segment_id, street, city, road_type, lat1, lon1, lat2, lon2\"\n",
    "segments_where = ''\n",
    "sql_segments = 'SELECT {} FROM segments_selected {}'.format(segments_select, segments_where)\n",
    "print('sql used to obtain segments dataframe:\\n' + sql_segments)\n",
    "\n",
    "segments_df = pd.read_sql(sql_segments, con=conn)\n",
    "print('segments dataframe has ' + str(len(segments_df))+\" rows\")\n",
    "\n",
    "# create time_df\n",
    "times_select = 'time_id, date, day_of_week, month, time'\n",
    "times_where = ''\n",
    "sql_time = 'SELECT {} FROM times_selected {}'.format(times_select, times_where)\n",
    "print('sql used to obtain time dataframe:\\n' + sql_time)\n",
    "\n",
    "time_df = pd.read_sql(sql_time, con=conn)\n",
    "print('time dataframe has ' + str(len(time_df))+\" rows\")\n",
    "\n",
    "# create cartesian product of segments and times to create segments_time_df\n",
    "time_df['tmp'] = 1\n",
    "segments_df['tmp'] = 1\n",
    "segments_time_df = pd.merge(time_df, segments_df, how='outer', on=['tmp'])\n",
    "print('cartesian product of segments and time has ' + str(len(segments_time_df))+\" rows\")\n",
    "\n",
    "# query database to get matrix of positive traffic incidents\n",
    "sql_matrix = '''\n",
    "select m.segment_id, m.time_id, s.street, s.lat1, s.lon1, \n",
    "    s.lat2, s.lon2, t.date, t.time, t.day_of_week, \n",
    "    s.road_type, s.city,\n",
    "    min(u.level) as level_min,\n",
    "    max(u.level) as level_max,\n",
    "    avg(u.level) as level_mean,\n",
    "    count(u.level) as level_count\n",
    "from matrix_''' + str(file_args['time_resolution']) + ''' m, times_selected t, uuid u, segments_selected s\n",
    "where m.time_id = t.time_id \n",
    "    and m.uuid_instance_id = u.uuid_instance_id \n",
    "    and s.segment_id = m.segment_id\n",
    "group by m.segment_id, m.time_id, s.street, s.lat1, s.lon1, \n",
    "    s.lat2, s.lon2, t.date, t.time, t.day_of_week, \n",
    "    s.road_type, s.city\n",
    "'''\n",
    "\n",
    "level_df = pd.read_sql(sql_matrix, con=conn)\n",
    "print('level dataframe has ' + str(len(level_df))+\" rows\")\n",
    "\n",
    "# join positive incidents to cartesian product of segments and times\n",
    "segments_time_level_df = pd.merge(segments_time_df, level_df[['segment_id','date','time','level_min','level_max','level_mean','level_count']], how='left', on=['segment_id','date','time'])\n",
    "print('joined segments/time/level dataframe has ' + str(len(segments_time_level_df))+\" rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check duplicates\n",
    "df = level_df\n",
    "print('{} duplicate rows'.format(str(sum(df.duplicated()))))\n",
    "df_row_counts = df.groupby(df.columns.tolist(), as_index=False).size()\n",
    "df_row_counts[df_row_counts.values > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check duplicates - they are coming from same segments having multiple road_types\n",
    "df = level_df[['segment_id','time','date','road_type']]\n",
    "print('{} duplicate rows'.format(str(sum(df.duplicated()))))\n",
    "df_row_counts = df.groupby(df.columns.tolist(), as_index=False).size()\n",
    "df_row_counts[df_row_counts.values > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_df_row_counts = segments_df.groupby(['segment_id','road_type'], as_index=False).size()\n",
    "segments_df_row_counts[segments_df_row_counts.values > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fillna with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Replace na values with zeros for assumption of no congestion\n",
    "level_cols = [c for c in segments_time_level_df.columns if c.startswith('level')]\n",
    "\n",
    "for c in level_cols:\n",
    "    segments_time_level_df[c].fillna(0, inplace=True)\n",
    "\n",
    "# segments_time_level_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process the data:\n",
    "1. add 'level_binary' column\n",
    "1. set 'time' column\n",
    "1. add number of days since earliest date\n",
    "1. add number of minutes since midnight\n",
    "1. encode categorical data to numeric using sklearn's labelencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 'target' column and set it to appropriate value based on input\n",
    "print('creating level_binary column...')\n",
    "segments_time_level_df['level_binary'] = segments_time_level_df[['tmp','level_count']].min(axis=1)\n",
    "\n",
    "# add date_idx for number of days since earliest date\n",
    "print('adding date_idx for number of days since earliest date...')\n",
    "td = pd.to_datetime(segments_time_level_df['date']) - pd.to_datetime(segments_time_level_df.date.min())\n",
    "date_idx_vals = (td / np.timedelta64(1, 'D')).astype(int)\n",
    "segments_time_level_df.loc[:,'date_idx'] = date_idx_vals\n",
    "\n",
    "# add time_idx for number of minutes since midnight\n",
    "print('adding time_idx for number of minutes since midnight...')\n",
    "time_idx_vals = map(lambda t: t.hour*60 + t.minute, segments_time_level_df['time'].values)\n",
    "segments_time_level_df.loc[:,'time_idx'] = time_idx_vals\n",
    "\n",
    "# define features and target\n",
    "print('subsetting data to date, time, features and target...')\n",
    "features = ['date_idx','time_idx','day_of_week','segment_id','street','city','road_type','lat1','lon1','lat2','lon2']\n",
    "targets = [c for c in segments_time_level_df.columns if c.startswith('level')]\n",
    "\n",
    "# subset data - include date, time, features, and target\n",
    "segments_time_level_df = segments_time_level_df.loc[:,['date','time'] + features + targets]\n",
    "\n",
    "# encode categorical data using label encoder - do not encode date and time\n",
    "print('encoding categorical columns as numeric...')\n",
    "num_cols = segments_time_level_df._get_numeric_data().columns\n",
    "cat_cols = list(set(segments_time_level_df.columns) - set(num_cols) - {'date','time'})\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "for col in cat_cols:\n",
    "    print 'processing {} column'.format(col)\n",
    "    segments_time_level_df[col] = le.fit_transform(segments_time_level_df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--- data processing took {0:.1f} seconds ---'.format(time.time() - data_processing_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. add features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "add_features_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_event_title(e):\n",
    "    e_clean = e.translate(string.maketrans(\"\",\"\"), string.punctuation).replace(' ','_')\n",
    "    return e_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if file_args['add_events']:\n",
    "    # get events from dataframe\n",
    "    events_df = pd.read_sql('SELECT * FROM events', con=conn)\n",
    "    events_df['event_start'] = pd.to_datetime(events_df['event_start'])\n",
    "    events_df['event_end'] = pd.to_datetime(events_df['event_end'])\n",
    "    \n",
    "    # subset to events larger than event_attendance_threshold\n",
    "    events_of_interest = events_df[events_df['exp_attendance']>=file_args['event_attendance_threshold']]\n",
    "\n",
    "    # add event durations\n",
    "    #event_durations = events_of_interest['event_end'] - events_of_interest['event_start']\n",
    "    #events_of_interest.loc[:,'duration'] = event_durations.values\n",
    "\n",
    "    # add datetime column to data\n",
    "    segments_time_level_df['datetime'] = segments_time_level_df[['date','time']].apply(lambda row: dt.datetime.combine(row['date'], row['time']), axis=1)\n",
    "    \n",
    "    # add event columns to data\n",
    "    event_features = segments_time_level_df[['datetime']].copy()\n",
    "    \n",
    "    # add columns for events\n",
    "    segments_time_level_df = pd.concat(\n",
    "        [\n",
    "            segments_time_level_df,\n",
    "            pd.DataFrame(\n",
    "                index=event_features.index, \n",
    "                columns=['event_{}'.format(clean_event_title(e)) for e in events_of_interest['event_title'].unique()]\n",
    "            )\n",
    "        ], axis=1\n",
    "    ).fillna(0)\n",
    "    \n",
    "    # loop through events_df and set values for that event column to 1 if event was active\n",
    "    window = file_args['event_active_buffer'] # hours\n",
    "    for index, row in events_of_interest.iterrows():\n",
    "        event = 'event_{}'.format(clean_event_title(row['event_title']))\n",
    "        start = row['event_start'] - dt.timedelta(hours=1)\n",
    "        end = row['event_end'] + dt.timedelta(hours=1)\n",
    "        segments_time_level_df.loc[(segments_time_level_df['datetime']>=start) & (segments_time_level_df['datetime']<=end), event] = 1\n",
    "    \n",
    "    # drop added datetime column\n",
    "    segments_time_level_df.drop('datetime', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--- adding events took {0:.1f} seconds ---'.format(time.time() - add_features_start))\n",
    "add_padres_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if file_args['add_padres']:\n",
    "    # get padres from database\n",
    "    padres_df = pd.read_sql('SELECT * FROM padres_games', con=conn)\n",
    "    padres_df['game_start'] = pd.to_datetime(padres_df['game_start'])\n",
    "    padres_df['game_end'] = pd.to_datetime(padres_df['game_end'])\n",
    "\n",
    "    # add datetime column to data\n",
    "    segments_time_level_df['datetime'] = segments_time_level_df[['date','time']].apply(lambda row: dt.datetime.combine(row['date'], row['time']), axis=1)\n",
    "    \n",
    "    # add padres_game column to data\n",
    "    segments_time_level_df.loc[:,'padres_game'] = 0\n",
    "    \n",
    "    # set values for padres_event column to 1 if padres game was occurring\n",
    "    padres_start_window_before = file_args['padres_start_window_before'] # 2 # hours\n",
    "    padres_start_window_after = file_args['padres_start_window_after'] #0.5 # hours\n",
    "    padres_end_window_before = file_args['padres_end_window_before'] #0.5 # hours\n",
    "    padres_end_window_after = file_args['padres_end_window_after'] #1 # hours\n",
    "\n",
    "    for index, row in padres_df.iterrows():\n",
    "        # set active before/after game start time\n",
    "        start = row['game_start'] - dt.timedelta(hours=padres_start_window_before)\n",
    "        end = row['game_start'] + dt.timedelta(hours=padres_start_window_after)\n",
    "        segments_time_level_df.loc[(segments_time_level_df['datetime']>=start) & \n",
    "                                   (segments_time_level_df['datetime']<=end), 'padres_game'] = 1\n",
    "\n",
    "        # set active before/after game end time\n",
    "        start = row['game_end'] - dt.timedelta(hours=padres_end_window_before)\n",
    "        end = row['game_end'] + dt.timedelta(hours=padres_end_window_after)\n",
    "        segments_time_level_df.loc[(segments_time_level_df['datetime']>=start) & \n",
    "                                   (segments_time_level_df['datetime']<=end), 'padres_game'] = 1\n",
    "    \n",
    "    # drop added datetime column\n",
    "    segments_time_level_df.drop('datetime', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--- adding padres games took {0:.1f} seconds ---'.format(time.time() - add_padres_start))\n",
    "print('--- adding all features took {0:.1f} seconds ---'.format(time.time() - add_features_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create data structure with params and data and write to pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if file_args['write_pickle_file']:\n",
    "    data_to_write = {\n",
    "        'parameters': file_args,\n",
    "        'data': segments_time_level_df\n",
    "    }\n",
    "    \n",
    "    filename_to_write = file_args['output_file']['filename_base']\n",
    "    print('writing data to pickle file - {}...'.format(filename_to_write))\n",
    "    if aws:\n",
    "        print('saving pickle file to s3')\n",
    "        s3 = boto3.resource('s3')\n",
    "        aws.save_file(filename_to_write, data_to_write)\n",
    "    else:\n",
    "        print('saving pickle file to local disk')\n",
    "        pickle.dump(data_to_write, open(filename_to_write, 'wb'))\n",
    "    print('pickle file dump complete...')\n",
    "else:\n",
    "    print('not writing pickle file...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test_split_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('splitting train and test data...')\n",
    "train_data_random, test_data_random = util.process_train_test(segments_time_level_df, 'random')\n",
    "train_data_date, test_data_date = util.process_train_test(segments_time_level_df, 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--- train test split took {0:.1f} seconds ---'.format(time.time() - train_test_split_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clustering_start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 - clustering on random train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusters_rand_sparse = Cluster(conn, file_args, train_data_random, 'random', file_args['num_clusters']['sparse'])\n",
    "clusters_rand_nonsparse = Cluster(conn, file_args, train_data_random, 'random', file_args['num_clusters']['nonsparse'])\n",
    "clusters_rand_sparse_long = Cluster(conn, file_args, train_data_random, 'random', file_args['num_clusters']['sparse_long'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 - sparse clustering - train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_model, sparse_clusters = clusters_rand_sparse.train_clustermodel_sparse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_random = train_data_random.merge(sparse_clusters, how='left', on='segment_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 - nonsparse clustering - train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonsparse_model, nonsparse_clusters = clusters_rand_nonsparse.train_clustermodel_nonsparse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_random = train_data_random.merge(nonsparse_clusters, how='left', on=['date','time','segment_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3 - sparse long clustering - train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_long_model, sparse_long_clusters = clusters_rand_sparse_long.train_clustermodel_sparse_long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_random = train_data_random.merge(sparse_long_clusters, how='left', on='segment_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.4 - sparse clustering - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.date.today()\n",
    "filename = file_args['cluster_algorithm'] + '_sparse_cluster_model_' + today.strftime('%Y%m%d') + '.pkl'\n",
    "test_clusters_sparse = clusters_rand_sparse.test_assign_clusters_sparse(test_data_random, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data_random = test_data_random.merge(test_clusters_sparse, how='left', on='segment_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.5 - sparse long clustering - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.date.today()\n",
    "filename = file_args['cluster_algorithm'] + '_sparse_long_cluster_model_' + today.strftime('%Y%m%d') + '.pkl'\n",
    "test_clusters_sparse_long = clusters_rand_sparse_long.test_assign_clusters_sparse_long(test_data_random, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data_random = test_data_random.merge(test_clusters_sparse_long, how='left', on='segment_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.6 - nonsparse clustering - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = file_args['cluster_algorithm'] + '_nonsparse_cluster_model_' + today.strftime('%Y%m%d') + '.pkl'\n",
    "test_clusters_nonsparse = clusters_rand_nonsparse.test_assign_clusters_nonsparse(test_data_random, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data_random = test_data_random.merge(test_clusters_nonsparse, how='left', on=['date','time','segment_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 - clustering on train/test split by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusters_date_sparse = Cluster(conn, file_args, train_data_date, 'date', file_args['num_clusters']['sparse'])\n",
    "clusters_date_nonsparse = Cluster(conn, file_args, train_data_date, 'date', file_args['num_clusters']['nonsparse'])\n",
    "clusters_date_sparse_long = Cluster(conn, file_args, train_data_date, 'date', file_args['num_clusters']['sparse_long'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 - sparse clustering - train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_model, sparse_clusters = clusters_date_sparse.train_clustermodel_sparse()\n",
    "train_data_date = train_data_date.merge(sparse_clusters, how='left', on='segment_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 - sparse clustering - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = file_args['cluster_algorithm'] + '_sparse_cluster_model_' + today.strftime('%Y%m%d') + '.pkl'\n",
    "test_clusters_sparse = clusters_date_sparse.test_assign_clusters_sparse(test_data_date, filename)\n",
    "test_data_date = test_data_date.merge(test_clusters_sparse, how='left', on='segment_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3 - nonsparse clustering - train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonsparse_model, nonsparse_clusters = clusters_date_nonsparse.train_clustermodel_nonsparse()\n",
    "train_data_date = train_data_date.merge(nonsparse_clusters, how='left', on=['date','time','segment_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.4 - nonsparse clustering - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = file_args['cluster_algorithm'] + '_nonsparse_cluster_model_' + today.strftime('%Y%m%d') + '.pkl'\n",
    "test_clusters_nonsparse = clusters_date_nonsparse.test_assign_clusters_nonsparse(test_data_date, filename)\n",
    "test_data_date = test_data_date.merge(test_clusters_nonsparse, how='left', on=['date','time','segment_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.5 - sparse long clustering - train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_long_model, sparse_long_clusters = clusters_date_sparse_long.train_clustermodel_sparse_long()\n",
    "train_data_date = train_data_date.merge(sparse_long_clusters, how='left', on='segment_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.5 - sparse long clustering - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = file_args['cluster_algorithm'] + '_sparse_long_cluster_model_' + today.strftime('%Y%m%d') + '.pkl'\n",
    "test_clusters_sparse_long = clusters_date_sparse_long.test_assign_clusters_sparse_long(test_data_date, filename)\n",
    "test_data_date = test_data_date.merge(test_clusters_sparse_long, how='left', on='segment_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_random.rename(columns={'cluster_sparse':'cluster_sparse_random'}, inplace=True)\n",
    "test_data_random.rename(columns={'cluster_sparse':'cluster_sparse_random'}, inplace=True)\n",
    "train_data_random.rename(columns={'cluster_sparse_long':'cluster_sparse_long_random'}, inplace=True)\n",
    "test_data_random.rename(columns={'cluster_sparse_long':'cluster_sparse_long_random'}, inplace=True)\n",
    "train_data_random.rename(columns={'cluster_nonsparse':'cluster_nonsparse_random'}, inplace=True)\n",
    "test_data_random.rename(columns={'cluster_nonsparse':'cluster_nonsparse_random'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_date.rename(columns={'cluster_sparse':'cluster_sparse_date'}, inplace=True)\n",
    "test_data_date.rename(columns={'cluster_sparse':'cluster_sparse_date'}, inplace=True)\n",
    "train_data_date.rename(columns={'cluster_sparse_long':'cluster_sparse_long_date'}, inplace=True)\n",
    "test_data_date.rename(columns={'cluster_sparse_long':'cluster_sparse_long_date'}, inplace=True)\n",
    "train_data_date.rename(columns={'cluster_nonsparse':'cluster_nonsparse_date'}, inplace=True)\n",
    "test_data_date.rename(columns={'cluster_nonsparse':'cluster_nonsparse_date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--- clustering took {0:.1f} seconds ---'.format(time.time() - clustering_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - concatenate train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add train_test columns\n",
    "train_data_random['train_test_random'] = 'train'\n",
    "test_data_random['train_test_random'] = 'test'\n",
    "\n",
    "train_data_date['train_test_date'] = 'train'\n",
    "test_data_date['train_test_date'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test_date = pd.concat([train_data_date, test_data_date])\n",
    "train_test_random = pd.concat([train_data_random, test_data_random])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test = train_test_date.merge(train_test_random[['date_idx','time_idx','segment_id',\n",
    "                                                      'cluster_sparse_random','cluster_nonsparse_random',\n",
    "                                                      'cluster_sparse_long_random',\n",
    "                                                      'train_test_random']],\n",
    "                                  how='left', on=['date_idx','time_idx','segment_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write processed dataframe to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writing_to_db_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write processed dataframe to database\n",
    "sqlalchemy_conn_str = open(sqlalchemy_conn_str_file, 'r').read()\n",
    "engine = create_engine(sqlalchemy_conn_str, paramstyle='format')\n",
    "train_test.to_sql(name='modeling_data', con=engine, \n",
    "                  if_exists='replace', index=False, chunksize=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--- writing to db took {0:.1f} seconds ---'.format(time.time() - writing_to_db_start))\n",
    "print('--- full data processing took {0:.1f} seconds ---'.format(time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "Phase 1 - Averages.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
