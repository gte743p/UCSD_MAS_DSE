{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy Metrics\n",
    "\n",
    "1. get predictions data\n",
    "2. calculate accuracy metrics by cluster for each modeling approach\n",
    "3. plot confusion matrix for each modeling approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import joblib\n",
    "import ast\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2 as pg\n",
    "import datetime as dt\n",
    "from AWS import AWS\n",
    "from Utility import Utility\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set environment\n",
    "aws = None\n",
    "s3_bucket_name = 'dse-cohort3-group3'\n",
    "s3_dat_dir = 'PreprocessedWazeData'\n",
    "\n",
    "# get args file\n",
    "sampling_args_file = '../conf/pipeline_args.txt'\n",
    "fr = open(sampling_args_file, 'r')\n",
    "fa = fr.read()\n",
    "file_args = ast.literal_eval(fa)\n",
    "\n",
    "# assume save_dir already exists\n",
    "save_dir = file_args['save_dir']\n",
    "\n",
    "# assume connection file is always present\n",
    "conn_str_file = file_args['conn_str_file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create subdirectory for results files - assumes save_dir already exists\n",
    "results_dir = '{}/results_files'.format(save_dir)\n",
    "\n",
    "# delete and remake results_dir if exists\n",
    "if os.path.isdir(results_dir):\n",
    "    shutil.rmtree(results_dir)\n",
    "\n",
    "os.mkdir(results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create utility object and connect to database\n",
    "util = Utility(file_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. read in actual and predictions for train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get actuals and predictions from csv\n",
    "fn = os.path.join(save_dir, 'train_predictions.csv')\n",
    "train_preds = pd.read_csv(fn)\n",
    "train_preds['date'] = pd.to_datetime(train_preds['date'],format='%Y-%m-%d').dt.date\n",
    "train_preds['time'] = pd.to_datetime(train_preds['time'],format='%H:%M:%S').dt.time\n",
    "print(str(len(train_preds))+\" rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get actuals and predictions from csv\n",
    "fn = os.path.join(save_dir, 'test_predictions.csv')\n",
    "test_preds = pd.read_csv(fn)\n",
    "test_preds['date'] = pd.to_datetime(test_preds['date'],format='%Y-%m-%d').dt.date\n",
    "test_preds['time'] = pd.to_datetime(test_preds['time'],format='%H:%M:%S').dt.time\n",
    "print(str(len(test_preds))+\" rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  create metrics tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pct_negative(data):\n",
    "    num_total = data.shape[0]\n",
    "    num_pos = data['level_binary'].sum()\n",
    "    num_neg = num_total - num_pos\n",
    "    neg_pct = num_neg / num_total * 1.0\n",
    "    return round(neg_pct,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_accuracy_metrics(data, target, model):\n",
    "    metrics_dict = {\n",
    "        'target': [],\n",
    "        'model': [],\n",
    "        'cluster': [],\n",
    "        'count': [],\n",
    "        'pct_negative': [],\n",
    "        'accuracy': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1_score': []\n",
    "    }\n",
    "    \n",
    "    # fillna with 0\n",
    "    data.fillna(0, inplace=True)\n",
    "    \n",
    "    # add additional column for best model if model is 'cluster_ensemble'\n",
    "    if model == 'cluster_ensemble':\n",
    "        metrics_dict['best_model'] = []\n",
    "        \n",
    "    # add individual cluster metrics\n",
    "    for clust in data['cluster'].unique():\n",
    "        clust_data = data[data['cluster']==clust]\n",
    "        clust_count = clust_data.shape[0]\n",
    "        clust_pct_neg = get_pct_negative(clust_data)\n",
    "        clust_y_true = clust_data[target].values\n",
    "        clust_y_pred = clust_data['{}_preds_{}'.format(target, model)]\n",
    "        clust_accuracy = round(accuracy_score(clust_y_true, clust_y_pred), 3)\n",
    "        clust_precision = round(precision_score(clust_y_true, clust_y_pred, average='macro'), 3)\n",
    "        clust_recall = round(recall_score(clust_y_true, clust_y_pred, average='macro'), 3)\n",
    "        clust_f1 = round(f1_score(clust_y_true, clust_y_pred, average='macro'), 3)\n",
    "\n",
    "        metrics_dict['target'].append(target)\n",
    "        metrics_dict['model'].append(model)\n",
    "        metrics_dict['cluster'].append(clust)\n",
    "        metrics_dict['count'].append(clust_count)\n",
    "        metrics_dict['pct_negative'].append(clust_pct_neg)\n",
    "        metrics_dict['accuracy'].append(clust_accuracy)\n",
    "        metrics_dict['precision'].append(clust_precision)\n",
    "        metrics_dict['recall'].append(clust_recall)\n",
    "        metrics_dict['f1_score'].append(clust_f1)\n",
    "        \n",
    "        if model=='cluster_ensemble':\n",
    "            stage = 1 if target=='level_binary' else 2\n",
    "            metrics_dict['best_model'].append(util.get_best_model(stage, clust)[1].replace('model_',''))\n",
    "\n",
    "    # add overall metrics\n",
    "    overall_y_true = data[target].values\n",
    "    overall_y_pred = data['{}_preds_{}'.format(target, model)]\n",
    "    overall_accuracy = round(accuracy_score(overall_y_true, overall_y_pred), 3)\n",
    "    overall_precision = round(precision_score(overall_y_true, overall_y_pred, average='macro'), 3)\n",
    "    overall_recall = round(recall_score(overall_y_true, overall_y_pred, average='macro'), 3)\n",
    "    overall_f1 = round(f1_score(overall_y_true, overall_y_pred, average='macro'), 3)\n",
    "    \n",
    "    metrics_dict['target'].append(target)\n",
    "    metrics_dict['model'].append(model)\n",
    "    metrics_dict['cluster'].append('all')\n",
    "    metrics_dict['count'].append(data.shape[0])\n",
    "    metrics_dict['pct_negative'].append(get_pct_negative(data))  \n",
    "    metrics_dict['accuracy'].append(overall_accuracy)\n",
    "    metrics_dict['precision'].append(overall_precision)\n",
    "    metrics_dict['recall'].append(overall_recall)\n",
    "    metrics_dict['f1_score'].append(overall_f1)\n",
    "    \n",
    "    if model=='cluster_ensemble':\n",
    "        metrics_dict['best_model'].append('N/A')\n",
    "\n",
    "    # create dataframe to return\n",
    "    metrics_df = pd.DataFrame.from_dict(metrics_dict)\n",
    "    if model=='cluster_ensemble':\n",
    "        metrics_df = metrics_df[['target','model','cluster','best_model','count',\n",
    "                                 'pct_negative','accuracy','precision','recall','f1_score']]\n",
    "    else:\n",
    "        metrics_df = metrics_df[['target','model','cluster','count','pct_negative',\n",
    "                                 'accuracy','precision','recall','f1_score']]\n",
    "\n",
    "    return metrics_df.sort_values(by='count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_accuracy_metrics(train_preds, 'level_binary', 'avg_baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_accuracy_metrics(test_preds, 'level_binary', 'avg_baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_accuracy_metrics(train_preds, 'level_binary', 'cluster_ensemble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_accuracy_metrics(test_preds, 'level_binary', 'cluster_ensemble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_accuracy_metrics(train_preds, 'level_max', 'avg_baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_accuracy_metrics(test_preds, 'level_max', 'avg_baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_accuracy_metrics(train_preds, 'level_max', 'cluster_ensemble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_accuracy_metrics(test_preds, 'level_max', 'cluster_ensemble')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  confusion matrix plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion Matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 1.1 if normalize else cm.max() / 1.2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.3f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.3f}; misclass={:0.3f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_cm(preds_df, target='level_max', model='avg_baseline', norm=False):\n",
    "    y_true = preds_df[target].values\n",
    "    y_pred = preds_df['{}_preds_{}'.format(target, model)].values\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_title = 'Confusion Matrix - {} target and {} model'.format(target, model)\n",
    "    plot_confusion_matrix(cm, target_names=None, title=cm_title, normalize=norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cm(preds_df=test_preds, target='level_binary', model='avg_baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cm(preds_df=test_preds, target='level_binary', model='cluster_ensemble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cm(preds_df=test_preds, target='level_max', model='avg_baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cm(preds_df=test_preds, target='level_max', model='cluster_ensemble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
