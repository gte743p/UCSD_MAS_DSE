{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation\n",
    "The purpose of this notebook is to transform the raw waze.csv data into a format matching our data schema. Before running this notebook, please run the following SQL script to create a new database:\n",
    "\n",
    "CREATE DATABASE waze_schema\n",
    "    WITH \n",
    "    OWNER = postgres\n",
    "    ENCODING = 'UTF8'\n",
    "    CONNECTION LIMIT = -1;\n",
    "\n",
    "CREATE EXTENSION postgis;\n",
    "CREATE EXTENSION postgis_topology;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2 as pg\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "import shapely\n",
    "import time\n",
    "from sqlalchemy import create_engine\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args_file = '../conf/pipeline_args.txt'\n",
    "fr = open(args_file, 'r')\n",
    "fa = fr.read()\n",
    "file_args = ast.literal_eval(fa)\n",
    "file_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlalchemy_conn_str = open('../conf/sqlalchemy_conn_str.txt', 'r').read()\n",
    "engine = create_engine(sqlalchemy_conn_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "postgres_password = open('../conf/postgres_password.txt', 'r').read()\n",
    "conn_str = \"host={} dbname={} user={} password={}\".format(\n",
    "    'localhost', 'waze_schema', 'postgres', (*your db username here*))\n",
    "\n",
    "conn = pg.connect(conn_str)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_bucket = str(file_args['time_resolution'])\n",
    "filepath = '../data/'\n",
    "cum_ts = 100/file_args['time_queries']['cum_ts_pct']\n",
    "cum_seg = 100/file_args['segment_queries']['cum_seg_pct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# allow processing the raw fcsv ile directly or importing the csv into the db first \n",
    "# then re-pulling the data for transformation.\n",
    "import_type = 'csv' # db or csv\n",
    "\n",
    "if import_type == 'csv':\n",
    "    csv_file = filepath + 'waze_data.csv'\n",
    "    waze_raw_df = pd.read_csv(csv_file)\n",
    "elif import_type == 'db':  # assumes postgresql\n",
    "    # assume connection file is always present\n",
    "    conn_str_file = 'db_conn_str.txt'\n",
    "    conn = pg.connect(pg_conn_str)\n",
    "    waze_raw_df = pd.read_sql('select  id, uuid, waze_timestamp, street, \\\n",
    "                                       start_node, end_node, city, length, delay, \\\n",
    "                                       speed, level, road_type, geom, \\\n",
    "                                       ST_AsText(geom) as linestring, \\\n",
    "                                       ST_NumPoints(geom) as linestring_length \\\n",
    "                               from congestion', con=conn)\n",
    "\n",
    "# make a copy of waze_raw_df\n",
    "waze_processed_df = waze_raw_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "waze_processed_df.insert(0, 'id', range(1, len(waze_processed_df)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#waze_processed_df = waze_processed_df[waze_processed_df['uuid']<=1000000].reset_index().drop(columns = ['index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Processed DataFrame with Additional Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "waze_processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract day of week, date, time, and timestamp rounded to 15 minute interval\n",
    "if import_type == 'csv':\n",
    "    waze_processed_df['dow'] = pd.to_datetime(waze_processed_df['waze_timestamp']).dt.dayofweek\n",
    "    waze_processed_df['month'] = pd.to_datetime(waze_processed_df['waze_timestamp']).dt.month\n",
    "    waze_processed_df['date'] = pd.to_datetime(waze_processed_df['waze_timestamp']).dt.date\n",
    "    waze_processed_df['time'] = pd.to_datetime(waze_processed_df['waze_timestamp']).dt.time\n",
    "    waze_processed_df['timestamp_round'] = pd.to_datetime(waze_processed_df['waze_timestamp']).apply(lambda dt: datetime.datetime(dt.year, dt.month, dt.day, dt.hour,5*(dt.minute // 5)))\n",
    "    waze_processed_df['time_round'] = pd.to_datetime(waze_processed_df['timestamp_round']).dt.time\n",
    "elif import_type == 'db':\n",
    "    waze_processed_df['dow'] = waze_processed_df['waze_timestamp'].dt.dayofweek\n",
    "    waze_processed_df['month'] = waze_processed_df['waze_timestamp'].dt.month\n",
    "    waze_processed_df['date'] = waze_processed_df['waze_timestamp'].dt.date\n",
    "    waze_processed_df['time'] = waze_processed_df['waze_timestamp'].dt.time\n",
    "    waze_processed_df['timestamp_round'] = waze_processed_df['waze_timestamp'].apply(lambda dt: datetime.datetime(dt.year, dt.month, dt.day, dt.hour,5*(dt.minute // 5)))\n",
    "    waze_processed_df['time_round'] = waze_processed_df['timestamp_round'].dt.time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "waze_processed_df['is_weekend'] = np.where(((waze_processed_df['dow']==5) | (waze_processed_df['dow']==6)), 1, None)\n",
    "\n",
    "holidays = [datetime.date(2017,1,2), datetime.date(2017,1,16), datetime.date(2017,2,20), datetime.date(2017,5,29)]\n",
    "holidays_df = pd.DataFrame(holidays, columns = ['date'])\n",
    "holidays_df['is_holiday'] = 1\n",
    "waze_processed_df = pd.merge(waze_processed_df, holidays_df, how = 'left', on= 'date')\n",
    "\n",
    "waze_processed_df['waze_timestamp_tmp'] = pd.to_datetime(waze_processed_df['waze_timestamp'])\n",
    "waze_processed_df['is_rushhour'] = np.where((waze_processed_df.waze_timestamp_tmp.dt.strftime('%H:%M:%S').between('07:00:00','09:00:00')) | (waze_processed_df.waze_timestamp_tmp.dt.strftime('%H:%M:%S').between('16:00:00','19:00:00')), 1, None)\n",
    "waze_processed_df = waze_processed_df.drop(columns = ['waze_timestamp_tmp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "waze_processed_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from shapely.geometry import LineString, shape\n",
    "from shapely.wkb import dumps, loads\n",
    "from shapely.wkt import dumps, loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linestring_length(row):\n",
    "    _,ls = row.split('(')\n",
    "    linestring = ls[:-1]\n",
    "    segments = linestring.split(',')\n",
    "    return len(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if import_type == 'csv':\n",
    "    waze_processed_df['linestring'] = \\\n",
    "        waze_processed_df['geom'].apply(lambda x: shapely.wkb.loads(x, hex=True).wkt)\n",
    "\n",
    "    waze_processed_df['linestring_length'] =  waze_processed_df['linestring'].apply(linestring_length)\n",
    "waze_processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "waze_processed_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('--- reading and processing csv results took {0:.1f} seconds ---'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Time Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_cols = ['timestamp_round','date','time_round','dow','month','is_weekend','is_holiday','is_rushhour']\n",
    "time_df = waze_processed_df.loc[:,time_cols].drop_duplicates().sort_values(by='timestamp_round').reset_index()\n",
    "time_df.drop('index', axis=1, inplace=True)\n",
    "time_df['time_id'] = time_df.index + 1\n",
    "time_df.columns = ['timestamp_round', 'date', 'time', 'day_of_week', 'month', 'is_weekend', 'is_holiday', 'is_rushhour', 'time_id']\n",
    "time_df = time_df[['date','day_of_week','month','is_weekend','is_holiday','is_rushhour','time','timestamp_round','time_id']]\n",
    "time_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Segment Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper function\n",
    "def extract_segments(ls):\n",
    "    '''function to extract individual lonlat segments from a linestring'''\n",
    "    ls_0 = ls.split('(')[-1:][0]\n",
    "    lonlats_str = ls_0[:-1].split(',')\n",
    "    lons = [float(ll.split()[0]) for ll in lonlats_str]\n",
    "    lats = [float(ll.split()[1]) for ll in lonlats_str]\n",
    "    lonlats = list(zip(lons, lats))\n",
    "    segments = [(lonlats[i],lonlats[i+1]) for i in range(len(lonlats)-1)]\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract segments from linestrings\n",
    "linestrings = np.array(waze_processed_df['linestring'].values)\n",
    "waze_segments = map(extract_segments, linestrings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check to make sure lengths align\n",
    "# segment_lengths = np.array(map(len, waze_segments)) + 1\n",
    "# linestring_lengths = np.array(waze_raw_df['linestring_length'].values)\n",
    "\n",
    "# print 'should be zero: {}'.format(np.average(linestring_lengths - segment_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add segments to processed dataframe\n",
    "waze_processed_df['segments'] = waze_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # check random rows to make sure linestrings and segments match\n",
    "# nonmatching_row_count = 0\n",
    "# for random_row in np.random.choice(waze_processed_df.index.values, size=1000, replace=False):\n",
    "#     rand_linestring = waze_processed_df.iloc[random_row]['linestring']\n",
    "#     rand_segments = waze_processed_df.iloc[random_row]['segments']\n",
    "#     if rand_segments == extract_segments(rand_linestring):\n",
    "#         continue\n",
    "#     else:\n",
    "#         nonmatching_row_count+=1\n",
    "#         print 'row {} does not match'.format(random_row)\n",
    "\n",
    "# print 'nonmatching rows: {}'.format(nonmatching_row_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get list of individual segments\n",
    "segments_list = waze_processed_df['segments'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flatten list\n",
    "flat_segments_list = [segment for segments in segments_list for segment in segments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get unique segments\n",
    "unique_segments = np.array(list(set(flat_segments_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create dict for dataframe\n",
    "segments_dict = {\n",
    "    'segment_id': np.array(range(len(unique_segments))) + 1,\n",
    "    'segment': [s for s in unique_segments],\n",
    "    'lat1': [s[0][1] for s in unique_segments],\n",
    "    'lon1': [s[0][0] for s in unique_segments],\n",
    "    'lat2': [s[1][1] for s in unique_segments],\n",
    "    'lon2': [s[1][0] for s in unique_segments]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create segment dataframe\n",
    "segment_df = pd.DataFrame(segments_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # TODO - add additional columns\n",
    "# segment_df['street'] = None\n",
    "# segment_df['city'] = None\n",
    "# segment_df['road_type'] = None\n",
    "# segment_df['geom'] = None\n",
    "# segment_df['length'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "segment_df_tmp = segment_df[['lat1','lon1','lat2','lon2','segment_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create UUID Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "waze_processed_df['uuid_instance_id'] = waze_processed_df.index\n",
    "uuid_df = waze_processed_df[['uuid', 'uuid_instance_id', 'street','start_node','end_node','waze_timestamp','city','length','delay','speed','level','road_type']]\n",
    "uuid_df.columns = ['uuid', 'uuid_instance_id', 'street_original', 'start_node', 'end_node', 'waze_timestamp', 'city_original', 'length_original', 'delay', 'speed', 'level', 'road_type_original']\n",
    "uuid_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Time/Segment/UUID Mapping Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matrix_df = pd.DataFrame(columns=['uuid','uuid_instance_id','segments','lat1','lon1','lat2','lon2','path'])\n",
    "\n",
    "for i in range(len(waze_processed_df)):\n",
    "    linestring = waze_processed_df['segments'][i] \n",
    "    tmp_df = pd.DataFrame(linestring) \n",
    "    tmp_df['uuid'] = waze_processed_df['uuid'][i] \n",
    "    tmp_df['uuid_instance_id'] = waze_processed_df['uuid_instance_id'][i]\n",
    "    tmp_df['timestamp_round'] = waze_processed_df['timestamp_round'][i]\n",
    "    tmp_df['segments'] = waze_processed_df['segments'][i]\n",
    "\n",
    "    lon1 = []\n",
    "    lat1 = []\n",
    "    lon2 = []\n",
    "    lat2 = []\n",
    "    for j in range(len(tmp_df)):\n",
    "        lon1.append(tmp_df[0][j][0])\n",
    "        lat1.append(tmp_df[0][j][1])\n",
    "        lon2.append(tmp_df[1][j][0])\n",
    "        lat2.append(tmp_df[1][j][1])\n",
    "    tmp_df['lon1'] = lon1\n",
    "    tmp_df['lat1'] = lat1\n",
    "    tmp_df['lon2'] = lon2\n",
    "    tmp_df['lat2'] = lat2\n",
    "\n",
    "    tmp_df.insert(0, 'path', range(len(tmp_df)))\n",
    "    tmp_df\n",
    "    matrix_df = matrix_df.append(tmp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matrix_df_tmp = pd.merge(matrix_df, time_df, on='timestamp_round')\n",
    "matrix_df_tmp2 = pd.merge(matrix_df_tmp, segment_df, on=['lat1','lon1','lat2','lon2'])\n",
    "matrix_df = matrix_df_tmp2[['uuid_instance_id', 'path', 'time_id', 'segment_id']]\n",
    "matrix_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Segments table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uuid_withseg = pd.merge(uuid_df, matrix_df[['segment_id','uuid_instance_id']], on=['uuid_instance_id'])\n",
    "segfields = uuid_withseg[['segment_id','street_original','city_original','road_type_original']].drop_duplicates()\n",
    "segments_df = pd.merge(segment_df_tmp, segfields, on='segment_id')\n",
    "segments_df.columns = ['lat1','lon1','lat2','lon2','segment_id','street','city','road_type']\n",
    "segments_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write DataFrames to Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_df.to_sql(name='time', con=engine, if_exists='replace', dtype={'is_rushhour': sqlalchemy.types.Boolean, \n",
    "                             'is_weekend':  sqlalchemy.types.Boolean,\n",
    "                             'is_holiday': sqlalchemy.types.Boolean,\n",
    "                             'day_of_week': sqlalchemy.types.String})\n",
    "segments_df.to_sql(name='segments', con=engine, if_exists='replace')\n",
    "uuid_df.to_sql(name='uuid', con=engine, if_exists='replace')\n",
    "matrix_df.to_sql(name='matrix', con=engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('--- reading, processing csv and inserting results into db took {0:.1f} seconds ---'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Padres Games DataFrame to Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "padres_df = pd.read_csv(filepath+'padreswindow.csv')\n",
    "padres_df = padres_df[['Date','start_time','Time','Attendance']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fix_date(d):\n",
    "    mon, day = d.split()[1:]\n",
    "    if len(day) == 1:\n",
    "        day = '0'+day\n",
    "    return '{} {} 2017'.format(mon, day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_timedelta(duration):\n",
    "    hrs_mins = [int(x) for x in duration.split(':')]\n",
    "    mins = 60*hrs_mins[0] + hrs_mins[1]\n",
    "    return pd.Timedelta(minutes = mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fixed_dates = padres_df['Date'].apply(fix_date)\n",
    "padres_df.loc[:,'Date'] = fixed_dates\n",
    "\n",
    "date_col = pd.to_datetime(padres_df['Date'])\n",
    "padres_df.loc[:,'Date'] = date_col\n",
    "\n",
    "start_time_col = pd.to_datetime(padres_df['start_time'], format='%H:%M').dt.time\n",
    "padres_df.loc[:,'start_time'] = start_time_col\n",
    "\n",
    "game_duration = padres_df['Time'].apply(create_timedelta)\n",
    "padres_df.loc[:,'game_duration'] = game_duration\n",
    "\n",
    "padres_df['game_start'] = padres_df[['Date','start_time']].apply(lambda row: datetime.datetime.combine(row['Date'], row['start_time']), axis=1)\n",
    "padres_df['game_end'] = padres_df['game_start'] + padres_df['game_duration']\n",
    "\n",
    "padres_df = padres_df[['game_start','game_end','Attendance']]\n",
    "padres_df.columns = ['game_start','game_end','attendance']\n",
    "\n",
    "padres_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "padres_df.to_sql(name='padres_games', con=engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exec(open(\"SQLQueries.py\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur.execute(SQL_drop_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur.execute(SQL_update_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur.execute(SQL_create_events_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur.execute(SQL_Time_Bucketing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur.execute(SQL_create_segments_times_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur.execute(SQL_pct_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur.execute(SQL_pct_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
