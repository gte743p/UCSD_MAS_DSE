{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Make Model Predictions on Test Data\n",
    "1. Get best model for each cluster  \n",
    "2. Predict binary and level traffic values\n",
    "3. Write predictions to database and csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2 as pg\n",
    "import datetime as dt\n",
    "from sklearn import preprocessing\n",
    "from collections import OrderedDict\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "import cPickle as pickle\n",
    "import gc\n",
    "import socket\n",
    "import boto3\n",
    "from boto.utils import get_instance_metadata\n",
    "import ast\n",
    "from Segments import Segments\n",
    "from Times import Times\n",
    "from Cluster import Cluster\n",
    "from Utility import Utility\n",
    "from AWS import AWS\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import shutil\n",
    "import joblib\n",
    "import string\n",
    "\n",
    "# clustering\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# modeling\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set environment\n",
    "aws = None\n",
    "s3_bucket_name = 'dse-cohort3-group3'\n",
    "s3_dat_dir = 'PreprocessedWazeData'\n",
    "\n",
    "# assume connection file is always present\n",
    "sampling_args_file = '../conf/pipeline_args.txt'\n",
    "\n",
    "fr = open(sampling_args_file, 'r')\n",
    "fa = fr.read()\n",
    "file_args = ast.literal_eval(fa)\n",
    "\n",
    "save_dir = file_args['save_dir']\n",
    "conn_str_file = file_args['conn_str_file']\n",
    "sqlalchemy_conn_str_file = file_args['sqlalchemy_conn_str_file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pg_conn_str = open(conn_str_file, 'r').read()\n",
    "conn = pg.connect(pg_conn_str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dicts to track test results\n",
    "test_results_dict = {\n",
    "    'stage_1': {},\n",
    "    'stage_2': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "util = Utility(file_args)\n",
    "util.conn = conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = util.get_modeling_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--- getting data took {0:.1f} seconds ---'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.  first stage - level_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_modeling_stage_start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stage 1 avg baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create prediction dataframes\n",
    "targets = [c for c in train_data.columns if c.startswith('level')]\n",
    "train_preds = train_data[['date','time','date_idx','time_idx','segment_id','day_of_week','cluster']+targets].copy()\n",
    "test_preds = test_data[['date','time','date_idx','time_idx','segment_id','day_of_week','cluster']+targets].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make test predictions - move to step 4\n",
    "if file_args['model_avg_baseline']:\n",
    "    print('making predictions using avg baseline model...')\n",
    "    print('target variable is {}'.format(file_args['target_first_stage']))\n",
    "    pred_suffix = '_preds_avg_baseline'\n",
    "    model_test_results = {}\n",
    "    \n",
    "    # load averages for making predictions\n",
    "    fn = os.path.join(save_dir, 'stage1_model_avg_baseline.pkl')\n",
    "    model = joblib.load(fn)\n",
    "    \n",
    "    # join predictions to train and test dataframes\n",
    "    train_preds_avg = train_data.merge(model, how='left', on=['time_idx','segment_id','day_of_week'])\n",
    "    test_preds_avg = test_data.merge(model, how='left', on=['time_idx','segment_id','day_of_week'])\n",
    "    train_preds_avg = train_preds_avg[['date_idx','time_idx','segment_id',file_args['target_first_stage']+pred_suffix]]\n",
    "    test_preds_avg = test_preds_avg[['date_idx','time_idx','segment_id',file_args['target_first_stage']+pred_suffix]]\n",
    "    \n",
    "    # fill null predictions with 0\n",
    "    train_preds_avg[file_args['target_first_stage']+pred_suffix].fillna(value=0, inplace=True)\n",
    "    test_preds_avg[file_args['target_first_stage']+pred_suffix].fillna(value=0, inplace=True)\n",
    "      \n",
    "    # add results to prediction dataframes\n",
    "    train_preds = train_preds.merge(train_preds_avg, how='left', on=['time_idx','date_idx','segment_id'])\n",
    "    test_preds = test_preds.merge(test_preds_avg, how='left', on=['time_idx','date_idx','segment_id'])\n",
    "    \n",
    "    # calculate scores for individual clusters\n",
    "    for clust in test_data['cluster'].unique():\n",
    "        tmp_preds = test_preds.loc[test_preds['cluster']==clust,file_args['target_first_stage']+pred_suffix]\n",
    "        tmp_actuals = test_preds.loc[test_preds['cluster']==clust,file_args['target_first_stage']]\n",
    "        model_test_results[clust] = f1_score(tmp_actuals, tmp_preds, average=file_args['scoring_metric'][3:])\n",
    "    \n",
    "    test_results_dict['stage_1']['model_avg_baseline'] = model_test_results\n",
    "    \n",
    "else:\n",
    "    print('not making predictions using avg baseline model...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stage 1 non-baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    'random_forest': RandomForestClassifier(random_state=file_args['seed']),\n",
    "    'knn': KNeighborsClassifier(),\n",
    "    'extra_trees': ExtraTreesClassifier(random_state=file_args['seed']),\n",
    "    'gradient_boosting': GradientBoostingClassifier(random_state=file_args['seed']),\n",
    "    'logistic_regression': LogisticRegression(random_state=file_args['seed']),\n",
    "    'gaussian_nb': GaussianNB()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_stage1_model_on_full(model_key):\n",
    "    if file_args['model_full_{}'.format(model_key)]:\n",
    "        pred_suffix = '_preds_full_{}'.format(model_key)\n",
    "        model_test_results = {}\n",
    "\n",
    "        print('splitting features and targets...')\n",
    "        level_cols = [c for c in train_data.columns if c.startswith('level')]\n",
    "        X_trn = train_data.drop(labels=['date','time','cluster']+level_cols, axis=1)\n",
    "        Y_trn = train_data.loc[:,file_args['target_first_stage']].values.ravel()\n",
    "        X_tst = test_data.drop(labels=['date','time','cluster']+level_cols, axis=1)\n",
    "        Y_tst = test_data.loc[:,file_args['target_first_stage']].values.ravel()\n",
    "\n",
    "        print('making predictions...')\n",
    "        fn = os.path.join(save_dir, 'stage1_model_full_{}.pkl'.format(model_key))\n",
    "        model = joblib.load(fn)\n",
    "        trn_preds = model.predict(X_trn)\n",
    "        tst_preds = model.predict(X_tst)\n",
    "\n",
    "        # add results to prediction dataframes\n",
    "        train_preds.loc[:,file_args['target_first_stage']+pred_suffix] = trn_preds\n",
    "        test_preds.loc[:,file_args['target_first_stage']+pred_suffix] = tst_preds\n",
    "\n",
    "        # calculate scores for individual clusters\n",
    "        for clust in test_data['cluster'].unique():\n",
    "            tmp_preds = test_preds.loc[test_preds['cluster']==clust,file_args['target_first_stage']+pred_suffix]\n",
    "            tmp_actuals = test_preds.loc[test_preds['cluster']==clust,file_args['target_first_stage']]\n",
    "            model_test_results[clust] = f1_score(tmp_actuals, tmp_preds, average=file_args['scoring_metric'][3:])\n",
    "\n",
    "        test_results_dict['stage_1']['model_full_{}'.format(model_key)] = model_test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_key in model_dict.keys():\n",
    "    # predict stage 1 using model on full data\n",
    "    if file_args['model_full_{}'.format(model_key)]:\n",
    "        print('predicting full data using {}'.format(model_key))\n",
    "        pred_suffix = '_preds_full_{}'.format(model_key)\n",
    "        model_test_results = {}\n",
    "\n",
    "        # split features and targets\n",
    "        level_cols = [c for c in train_data.columns if c.startswith('level')]\n",
    "        X_trn = train_data.drop(labels=['date','time','cluster']+level_cols, axis=1)\n",
    "        Y_trn = train_data.loc[:,file_args['target_first_stage']].values.ravel()\n",
    "        X_tst = test_data.drop(labels=['date','time','cluster']+level_cols, axis=1)\n",
    "        Y_tst = test_data.loc[:,file_args['target_first_stage']].values.ravel()\n",
    "\n",
    "        # load model and make predictions\n",
    "        fn = os.path.join(save_dir, 'stage1_model_full_{}.pkl'.format(model_key))\n",
    "        model = joblib.load(fn)\n",
    "        trn_preds = model.predict(X_trn)\n",
    "        tst_preds = model.predict(X_tst)\n",
    "\n",
    "        # add results to prediction dataframes\n",
    "        train_preds.loc[:,file_args['target_first_stage']+pred_suffix] = trn_preds\n",
    "        test_preds.loc[:,file_args['target_first_stage']+pred_suffix] = tst_preds\n",
    "\n",
    "        # calculate scores for individual clusters\n",
    "        for clust in test_data['cluster'].unique():\n",
    "            tmp_preds = test_preds.loc[test_preds['cluster']==clust,file_args['target_first_stage']+pred_suffix]\n",
    "            tmp_actuals = test_preds.loc[test_preds['cluster']==clust,file_args['target_first_stage']]\n",
    "            model_test_results[clust] = f1_score(tmp_actuals, tmp_preds, average=file_args['scoring_metric'][3:])\n",
    "\n",
    "        # add results to results dict\n",
    "        test_results_dict['stage_1']['model_full_{}'.format(model_key)] = model_test_results\n",
    "    \n",
    "    # predict stage 1 by cluster\n",
    "    if file_args['model_clusters_{}'.format(model_key)]:\n",
    "        print('predicting clusters using {}'.format(model_key))\n",
    "        pred_suffix = '_preds_cluster_{}'.format(model_key)\n",
    "        test_results_dict['stage_1']['model_clusters_{}'.format(model_key)] = {}\n",
    "\n",
    "        # create dataframes to append predictions\n",
    "        train_preds_ensemble = pd.DataFrame(columns=['time_idx','date_idx','segment_id',file_args['target_first_stage']+pred_suffix])\n",
    "        test_preds_ensemble = pd.DataFrame(columns=['time_idx','date_idx','segment_id',file_args['target_first_stage']+pred_suffix])\n",
    "\n",
    "        for clust in test_data['cluster'].unique():\n",
    "            model_test_results = {}\n",
    "\n",
    "            # subset data to cluster\n",
    "            train_clust = train_data[train_data['cluster']==clust].copy()\n",
    "            test_clust = test_data[test_data['cluster']==clust].copy()\n",
    "\n",
    "            # calculate negative to positive ratio for each cluster\n",
    "            trn_clust_ratio = util.get_neg_pos_ratio(train_clust)\n",
    "            tst_clust_ratio = util.get_neg_pos_ratio(test_clust)\n",
    "\n",
    "            # unskew individual clusters\n",
    "            if (file_args['unskew_train_clusters'] and trn_ratio > file_args['unskew_ratio']):\n",
    "                print('unskewing train data to negative positive ratio of {}...'.format(file_args['unskew_ratio']))\n",
    "                train_clust = util.unskew_data(train_clust, file_args['unskew_ratio'])\n",
    "            if (file_args['unskew_test'] and tst_ratio > file_args['unskew_ratio']):\n",
    "                print('unskewing test data to negative positive ratio of {}...'.format(file_args['unskew_ratio']))\n",
    "                test_clust = util.unskew_data(test_clust, file_args['unskew_ratio'])\n",
    "\n",
    "            # split features and targets\n",
    "            level_cols = [col for col in train_data.columns if col.startswith('level')]\n",
    "            X_trn = train_clust.drop(labels=['date','time','cluster']+level_cols, axis=1)\n",
    "            Y_trn = train_clust.loc[:,file_args['target_first_stage']].values.ravel()\n",
    "            X_tst = test_clust.drop(labels=['date','time','cluster']+level_cols, axis=1)\n",
    "            Y_tst = test_clust.loc[:,file_args['target_first_stage']].values.ravel()\n",
    "\n",
    "            # make predictions\n",
    "            fn = os.path.join(save_dir, 'stage1_model_clusters_{}_cluster_{}.pkl'.format(model_key, clust))\n",
    "            model = joblib.load(fn)\n",
    "            trn_preds = model.predict(X_trn)\n",
    "            tst_preds = model.predict(X_tst)\n",
    "\n",
    "            # calculate score\n",
    "            model_test_results = f1_score(Y_tst, tst_preds, average=file_args['scoring_metric'][3:])\n",
    "\n",
    "            # add results to dict\n",
    "            test_results_dict['stage_1']['model_clusters_{}'.format(model_key)][clust] = model_test_results\n",
    "\n",
    "            # create cluster prediction dataframes\n",
    "            train_clust_preds = train_clust[['date_idx','time_idx','segment_id']].copy()\n",
    "            test_clust_preds = test_clust[['date_idx','time_idx','segment_id']].copy()\n",
    "            train_clust_preds.loc[:,file_args['target_first_stage']+pred_suffix] = trn_preds\n",
    "            test_clust_preds.loc[:,file_args['target_first_stage']+pred_suffix] = tst_preds\n",
    "\n",
    "            # add cluster predictions to full ensemble predictions dataframe\n",
    "            train_preds_ensemble = train_preds_ensemble.append(train_clust_preds)\n",
    "            test_preds_ensemble = test_preds_ensemble.append(test_clust_preds)\n",
    "\n",
    "        # add prediction columns to dataframes\n",
    "        train_preds = train_preds.merge(train_preds_ensemble, how='left', \n",
    "                                        on=['date_idx','time_idx','segment_id'])\n",
    "        test_preds = test_preds.merge(test_preds_ensemble, how='left', \n",
    "                                      on=['date_idx','time_idx','segment_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_stage1_model_on_clusters(model_key):\n",
    "    if file_args['model_clusters_{}'.format(model_key)]:\n",
    "        pred_suffix = '_preds_cluster_{}'.format(model_key)\n",
    "        test_results_dict['stage_1']['model_clusters_{}'.format(model_key)] = {}\n",
    "\n",
    "        # create dataframes to append predictions\n",
    "        train_preds_ensemble = pd.DataFrame(columns=['time_idx','date_idx','segment_id',file_args['target_first_stage']+pred_suffix])\n",
    "        test_preds_ensemble = pd.DataFrame(columns=['time_idx','date_idx','segment_id',file_args['target_first_stage']+pred_suffix])\n",
    "\n",
    "        for clust in test_data['cluster'].unique():\n",
    "            print('making predictions for cluster {}...'.format(clust))\n",
    "\n",
    "            model_test_results = {}\n",
    "\n",
    "            # subset data to cluster\n",
    "            train_clust = train_data[train_data['cluster']==clust].copy()\n",
    "            test_clust = test_data[test_data['cluster']==clust].copy()\n",
    "\n",
    "            # calculate negative to positive ratio for each cluster\n",
    "            trn_clust_ratio = util.get_neg_pos_ratio(train_clust)\n",
    "            tst_clust_ratio = util.get_neg_pos_ratio(test_clust)\n",
    "\n",
    "            # unskew individual clusters\n",
    "            if (file_args['unskew_train_clusters'] and trn_ratio > file_args['unskew_ratio']):\n",
    "                print('unskewing train data to negative positive ratio of {}...'.format(file_args['unskew_ratio']))\n",
    "                train_clust = util.unskew_data(train_clust, file_args['unskew_ratio'])\n",
    "            if (file_args['unskew_test'] and tst_ratio > file_args['unskew_ratio']):\n",
    "                print('unskewing test data to negative positive ratio of {}...'.format(file_args['unskew_ratio']))\n",
    "                test_clust = util.unskew_data(test_clust, file_args['unskew_ratio'])\n",
    "\n",
    "            # split features and targets\n",
    "            level_cols = [col for col in train_data.columns if col.startswith('level')]\n",
    "            X_trn = train_clust.drop(labels=['date','time','cluster']+level_cols, axis=1)\n",
    "            Y_trn = train_clust.loc[:,file_args['target_first_stage']].values.ravel()\n",
    "            X_tst = test_clust.drop(labels=['date','time','cluster']+level_cols, axis=1)\n",
    "            Y_tst = test_clust.loc[:,file_args['target_first_stage']].values.ravel()\n",
    "\n",
    "            # make predictions\n",
    "            fn = os.path.join(save_dir, 'stage1_model_clusters_{}_cluster_{}.pkl'.format(model_key, clust))\n",
    "            model = joblib.load(fn)\n",
    "            trn_preds = model.predict(X_trn)\n",
    "            tst_preds = model.predict(X_tst)\n",
    "\n",
    "            # calculate score\n",
    "            model_test_results = f1_score(Y_tst, tst_preds, average=file_args['scoring_metric'][3:])\n",
    "\n",
    "            # add model val results to dict\n",
    "            test_results_dict['stage_1']['model_clusters_{}'.format(model_key)][clust] = model_test_results\n",
    "\n",
    "            # create cluster prediction dataframes\n",
    "            train_clust_preds = train_clust[['date_idx','time_idx','segment_id']].copy()\n",
    "            test_clust_preds = test_clust[['date_idx','time_idx','segment_id']].copy()\n",
    "            train_clust_preds.loc[:,file_args['target_first_stage']+pred_suffix] = trn_preds\n",
    "            test_clust_preds.loc[:,file_args['target_first_stage']+pred_suffix] = tst_preds\n",
    "\n",
    "            train_preds_ensemble = train_preds_ensemble.append(train_clust_preds)\n",
    "            test_preds_ensemble = test_preds_ensemble.append(test_clust_preds)\n",
    "\n",
    "        # add prediction columns to dataframes\n",
    "        train_preds = train_preds.merge(train_preds_ensemble, how='left', \n",
    "                                        on=['date_idx','time_idx','segment_id'])\n",
    "        test_preds = test_preds.merge(test_preds_ensemble, how='left', \n",
    "                                      on=['date_idx','time_idx','segment_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stage 1 best ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if file_args['model_clusters_ensemble']:\n",
    "    print('making predictions using best models on validation data for each cluster...')\n",
    "    print('target variable is {}'.format(file_args['target_first_stage']))\n",
    "    pred_suffix = '_preds_cluster_ensemble'\n",
    "    test_results_dict['stage_1']['model_clusters_ensemble'] = {}\n",
    "\n",
    "    # create dataframes to append predictions\n",
    "    train_preds_ensemble = pd.DataFrame(columns=['time_idx','date_idx','segment_id',file_args['target_first_stage']+pred_suffix])\n",
    "    test_preds_ensemble = pd.DataFrame(columns=['time_idx','date_idx','segment_id',file_args['target_first_stage']+pred_suffix])\n",
    "    \n",
    "    for clust in test_data['cluster'].unique():\n",
    "        print('making predictions for cluster {}...'.format(clust))\n",
    "\n",
    "        model_test_results = {}\n",
    "        \n",
    "        # subset data to cluster\n",
    "        train_clust = train_data[train_data['cluster']==clust].copy()\n",
    "        test_clust = test_data[test_data['cluster']==clust].copy()\n",
    "               \n",
    "        # calculate negative to positive ratio for each cluster\n",
    "        trn_clust_ratio = util.get_neg_pos_ratio(train_clust)\n",
    "        tst_clust_ratio = util.get_neg_pos_ratio(test_clust)\n",
    "        \n",
    "        # unskew individual clusters\n",
    "        if (file_args['unskew_train_clusters'] and trn_ratio > file_args['unskew_ratio']):\n",
    "            print('unskewing train data to negative positive ratio of {}...'.format(file_args['unskew_ratio']))\n",
    "            train_clust = util.unskew_data(train_clust, file_args['unskew_ratio'])\n",
    "        if (file_args['unskew_test'] and tst_ratio > file_args['unskew_ratio']):\n",
    "            print('unskewing test data to negative positive ratio of {}...'.format(file_args['unskew_ratio']))\n",
    "            test_clust = util.unskew_data(test_clust, file_args['unskew_ratio'])\n",
    "\n",
    "        # split features and targets\n",
    "        level_cols = [col for col in train_data.columns if col.startswith('level')]\n",
    "        X_trn = train_clust.drop(labels=['date','time','cluster']+level_cols, axis=1)\n",
    "        Y_trn = train_clust.loc[:,file_args['target_first_stage']].values.ravel()\n",
    "        X_tst = test_clust.drop(labels=['date','time','cluster']+level_cols, axis=1)\n",
    "        Y_tst = test_clust.loc[:,file_args['target_first_stage']].values.ravel()\n",
    "\n",
    "        # get best model and make predictions\n",
    "        model, model_type = util.get_best_model(1, clust)\n",
    "        print('best model for cluster {} is {}'.format(clust, model_type))\n",
    "\n",
    "        if model_type == 'model_avg_baseline':\n",
    "            # join average based predictions to train and test dataframes\n",
    "            train_clust_preds = train_clust.merge(model, how='left', on=['time_idx','segment_id','day_of_week'])\n",
    "            test_clust_preds = test_clust.merge(model, how='left', on=['time_idx','segment_id','day_of_week'])\n",
    "            \n",
    "            # fill null predictions with 0\n",
    "            train_clust_preds[file_args['target_first_stage']+'_preds_avg_baseline'].fillna(value=0, inplace=True)\n",
    "            test_clust_preds[file_args['target_first_stage']+'_preds_avg_baseline'].fillna(value=0, inplace=True)\n",
    "            \n",
    "            trn_preds = train_clust_preds[file_args['target_first_stage']+'_preds_avg_baseline'].values\n",
    "            tst_preds = test_clust_preds[file_args['target_first_stage']+'_preds_avg_baseline'].values            \n",
    "        else:\n",
    "            trn_preds = model.predict(X_trn)\n",
    "            tst_preds = model.predict(X_tst)\n",
    "\n",
    "        # calculate score and add to results\n",
    "        model_test_results = f1_score(Y_tst, tst_preds, average=file_args['scoring_metric'][3:])\n",
    "        test_results_dict['stage_1']['model_clusters_ensemble'][clust] = model_test_results\n",
    "        \n",
    "        # create cluster prediction dataframes\n",
    "        train_clust_preds = train_clust[['date_idx','time_idx','segment_id']].copy()\n",
    "        test_clust_preds = test_clust[['date_idx','time_idx','segment_id']].copy()\n",
    "        train_clust_preds.loc[:,file_args['target_first_stage']+pred_suffix] = trn_preds\n",
    "        test_clust_preds.loc[:,file_args['target_first_stage']+pred_suffix] = tst_preds\n",
    "        \n",
    "        train_preds_ensemble = train_preds_ensemble.append(train_clust_preds)\n",
    "        test_preds_ensemble = test_preds_ensemble.append(test_clust_preds)\n",
    "    \n",
    "    # add prediction columns to dataframes\n",
    "    train_preds = train_preds.merge(train_preds_ensemble, how='left', \n",
    "                                    on=['date_idx','time_idx','segment_id'])\n",
    "    test_preds = test_preds.merge(test_preds_ensemble, how='left', \n",
    "                                  on=['date_idx','time_idx','segment_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--- first stage predictions took {0:.1f} seconds ---'.format(time.time() - first_modeling_stage_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. second stage - multiclass level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "second_modeling_stage_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_pos = train_data[train_data['level_binary'] == 1]\n",
    "test_data_pos_preds = test_preds.loc[test_preds['level_binary_preds_cluster_ensemble'] == 1,\n",
    "                                    ['date_idx','time_idx','segment_id']]\n",
    "test_data_pos = test_data_pos_preds.merge(test_data, how='left', on=['date_idx','time_idx','segment_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stage 2 avg baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model to create test predictions \n",
    "if file_args['model_avg_baseline']:\n",
    "    print('making predictions using avg baseline model...')\n",
    "    print('target variable is {}'.format(file_args['target_second_stage']))\n",
    "    pred_suffix = '_preds_avg_baseline'\n",
    "    model_test_results = {}\n",
    "    \n",
    "    # load averages for making predictions\n",
    "    fn = os.path.join(save_dir, 'stage2_model_avg_baseline.pkl')\n",
    "    model = joblib.load(fn)\n",
    "    \n",
    "    # join predictions to train and test dataframes\n",
    "    train_preds_avg = train_data_pos.merge(model, how='left', on=['time_idx','segment_id','day_of_week'])\n",
    "    test_preds_avg = test_data_pos.merge(model, how='left', on=['time_idx','segment_id','day_of_week'])\n",
    "    train_preds_avg = train_preds_avg[['date_idx','time_idx','segment_id',file_args['target_second_stage']+pred_suffix]]\n",
    "    test_preds_avg = test_preds_avg[['date_idx','time_idx','segment_id',file_args['target_second_stage']+pred_suffix]]\n",
    "    \n",
    "    # add results to prediction dataframes\n",
    "    print('adding average based predictions to results...')\n",
    "    train_preds = train_preds.merge(train_preds_avg, how='left', on=['time_idx','date_idx','segment_id'])\n",
    "    test_preds = test_preds.merge(test_preds_avg, how='left', on=['time_idx','date_idx','segment_id'])\n",
    "\n",
    "    # fill null predictions with 0\n",
    "    print('filling null predictions to 0...')\n",
    "    train_preds[file_args['target_second_stage']+pred_suffix].fillna(value=0, inplace=True)\n",
    "    test_preds[file_args['target_second_stage']+pred_suffix].fillna(value=0, inplace=True)\n",
    "    \n",
    "    # calculate scores for individual clusters\n",
    "    for clust in test_data_pos['cluster'].unique():\n",
    "        tmp_preds = test_preds.loc[test_preds['cluster']==clust,file_args['target_second_stage']+pred_suffix]\n",
    "        tmp_actuals = test_preds.loc[test_preds['cluster']==clust,file_args['target_second_stage']]\n",
    "        model_test_results[clust] = f1_score(tmp_actuals, tmp_preds, average=file_args['scoring_metric'][3:])\n",
    "    \n",
    "    test_results_dict['stage_2']['model_avg_baseline'] = model_test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stage 2 non-baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_key in model_dict.keys():\n",
    "    # predict stage 2 full data\n",
    "    if file_args['model_full_{}'.format(model_key)]:\n",
    "        print('predicting full {}'.format(model_key))\n",
    "        pred_suffix = '_preds_full_{}'.format(model_key)\n",
    "        model_test_results = {}\n",
    "\n",
    "        # split features and targets\n",
    "        level_cols = [c for c in train_data_pos.columns if c.startswith('level')]\n",
    "        X_trn = train_data_pos.drop(labels=['date','time','cluster']+level_cols, axis=1)\n",
    "        Y_trn = train_data_pos.loc[:,file_args['target_second_stage']].values.ravel()\n",
    "        X_tst = test_data_pos.drop(labels=['date','time','cluster']+level_cols, axis=1)\n",
    "        Y_tst = test_data_pos.loc[:,file_args['target_second_stage']].values.ravel()\n",
    "\n",
    "        # load model and make predictions\n",
    "        fn = os.path.join(save_dir, 'stage2_model_full_{}.pkl'.format(model_key))\n",
    "        model = joblib.load(fn)\n",
    "        trn_preds = model.predict(X_trn)\n",
    "        tst_preds = model.predict(X_tst)\n",
    "\n",
    "        # create prediction dataframes\n",
    "        train_preds_pos = train_data_pos[['date_idx','time_idx','segment_id']].copy()\n",
    "        test_preds_pos = test_data_pos[['date_idx','time_idx','segment_id']].copy()\n",
    "        train_preds_pos.loc[:,file_args['target_second_stage']+pred_suffix] = trn_preds\n",
    "        test_preds_pos.loc[:,file_args['target_second_stage']+pred_suffix] = tst_preds\n",
    "\n",
    "        # join to predictions and fillna with 0\n",
    "        train_preds = train_preds.merge(train_preds_pos, how='left', on=['segment_id','date_idx','time_idx'])\n",
    "        test_preds = test_preds.merge(test_preds_pos, how='left', on=['segment_id','date_idx','time_idx'])\n",
    "        train_preds[file_args['target_second_stage']+pred_suffix].fillna(value=0, inplace=True)\n",
    "        test_preds[file_args['target_second_stage']+pred_suffix].fillna(value=0, inplace=True)\n",
    "\n",
    "        # calculate scores for individual clusters\n",
    "        for clust in test_data['cluster'].unique():\n",
    "            tmp_preds = test_preds.loc[test_preds['cluster']==clust,file_args['target_second_stage']+pred_suffix]\n",
    "            tmp_actuals = test_preds.loc[test_preds['cluster']==clust,file_args['target_second_stage']]\n",
    "            model_test_results[clust] = f1_score(tmp_actuals, tmp_preds, average=file_args['scoring_metric'][3:])\n",
    "\n",
    "        test_results_dict['stage_2']['model_full_{}'.format(model_key)] = model_test_results\n",
    "    \n",
    "    # predict stage 2 clusters\n",
    "    if file_args['model_clusters_{}'.format(model_key)]:\n",
    "        print('predicting cluster {}'.format(model_key))\n",
    "        pred_suffix = '_preds_cluster_{}'.format(model_key)\n",
    "        test_results_dict['stage_2']['model_clusters_{}'.format(model_key)] = {}\n",
    "\n",
    "        # create dataframes to append predictions\n",
    "        train_preds_ensemble = pd.DataFrame(columns=['time_idx','date_idx','segment_id',file_args['target_second_stage']+pred_suffix])\n",
    "        test_preds_ensemble = pd.DataFrame(columns=['time_idx','date_idx','segment_id',file_args['target_second_stage']+pred_suffix])\n",
    "\n",
    "        model_test_results = {}\n",
    "\n",
    "        for clust in test_data_pos['cluster'].unique():\n",
    "            # subset data to cluster\n",
    "            train_clust = train_data[train_data['cluster']==clust].copy()\n",
    "            test_clust = test_data[test_data['cluster']==clust].copy()\n",
    "\n",
    "            # calculate negative to positive ratio for each cluster\n",
    "            trn_clust_ratio = util.get_neg_pos_ratio(train_clust)\n",
    "            tst_clust_ratio = util.get_neg_pos_ratio(test_clust)\n",
    "\n",
    "            # subset to positive data\n",
    "            train_clust_pos = train_data_pos[train_data_pos['cluster']==clust].copy()\n",
    "            test_clust_pos = test_data_pos[test_data_pos['cluster']==clust].copy()\n",
    "            train_preds_pos = train_preds[train_preds['cluster']==clust].copy()\n",
    "            test_preds_pos = test_preds[test_preds['cluster']==clust].copy()\n",
    "\n",
    "            # split features and targets\n",
    "            level_cols = [col for col in train_data_pos.columns if col.startswith('level')]\n",
    "            X_trn = train_clust_pos.drop(labels=['date','time','cluster']+level_cols, axis=1)\n",
    "            Y_trn = train_clust_pos.loc[:,file_args['target_second_stage']].values.ravel()\n",
    "            X_tst = test_clust_pos.drop(labels=['date','time','cluster']+level_cols, axis=1)\n",
    "            Y_tst = test_clust_pos.loc[:,file_args['target_second_stage']].values.ravel()\n",
    "\n",
    "            # make predictions\n",
    "            fn = os.path.join(save_dir, 'stage2_model_clusters_{}_cluster_{}.pkl'.format(model_key, clust))\n",
    "            model = joblib.load(fn)\n",
    "            trn_preds = model.predict(X_trn)\n",
    "            tst_preds = model.predict(X_tst)\n",
    "\n",
    "            # create prediction dataframes\n",
    "            train_preds_pos = train_clust_pos[['date_idx','time_idx','segment_id']].copy()\n",
    "            test_preds_pos = test_clust_pos[['date_idx','time_idx','segment_id']].copy()\n",
    "            train_clust_preds = train_clust[['date_idx','time_idx','segment_id',file_args['target_second_stage']]].copy()\n",
    "            test_clust_preds = test_clust[['date_idx','time_idx','segment_id',file_args['target_second_stage']]].copy()\n",
    "\n",
    "            # add positive predictions\n",
    "            train_preds_pos.loc[:,file_args['target_second_stage']+pred_suffix] = trn_preds\n",
    "            test_preds_pos.loc[:,file_args['target_second_stage']+pred_suffix] = tst_preds\n",
    "\n",
    "            # join to predictions and fillna with 0\n",
    "            train_clust_preds = train_clust_preds.merge(train_preds_pos, how='left', on=['segment_id','date_idx','time_idx'])\n",
    "            test_clust_preds = test_clust_preds.merge(test_preds_pos, how='left', on=['segment_id','date_idx','time_idx'])\n",
    "            train_clust_preds[file_args['target_second_stage']+pred_suffix].fillna(value=0, inplace=True)\n",
    "            test_clust_preds[file_args['target_second_stage']+pred_suffix].fillna(value=0, inplace=True)\n",
    "\n",
    "            # calculate scores for individual cluster\n",
    "            tmp_preds = test_clust_preds[file_args['target_second_stage']+pred_suffix].values\n",
    "            tmp_actuals = test_clust_preds[file_args['target_second_stage']].values\n",
    "            model_test_results[clust] = f1_score(tmp_actuals, tmp_preds, average=file_args['scoring_metric'][3:])\n",
    "\n",
    "            # drop target column\n",
    "            train_clust_preds.drop(labels=file_args['target_second_stage'], axis=1, inplace=True)\n",
    "            test_clust_preds.drop(labels=file_args['target_second_stage'], axis=1, inplace=True)\n",
    "\n",
    "            # add results to ensemble dataframe\n",
    "            train_preds_ensemble = train_preds_ensemble.append(train_clust_preds)\n",
    "            test_preds_ensemble = test_preds_ensemble.append(test_clust_preds)\n",
    "\n",
    "        # add prediction columns to dataframes\n",
    "        train_preds = train_preds.merge(train_preds_ensemble, how='left', \n",
    "                                        on=['date_idx','time_idx','segment_id'])\n",
    "        test_preds = test_preds.merge(test_preds_ensemble, how='left', \n",
    "                                      on=['date_idx','time_idx','segment_id'])\n",
    "        # add results to dict\n",
    "        test_results_dict['stage_2']['model_clusters_{}'.format(model_key)] = model_test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stage 2 clusters with best ensemble models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if file_args['model_clusters_ensemble']:\n",
    "    print('making predictions using best models on validation data for each cluster...')\n",
    "    print('target variable is {}'.format(file_args['target_second_stage']))\n",
    "    pred_suffix = '_preds_cluster_ensemble'   \n",
    "    test_results_dict['stage_2']['model_clusters_ensemble'] = {}\n",
    "    model_test_results = {}\n",
    "    \n",
    "    # create dataframes to append predictions\n",
    "    train_preds_ensemble = pd.DataFrame(columns=['time_idx','date_idx','segment_id',file_args['target_second_stage']+pred_suffix])\n",
    "    test_preds_ensemble = pd.DataFrame(columns=['time_idx','date_idx','segment_id',file_args['target_second_stage']+pred_suffix])\n",
    "    \n",
    "    for clust in test_data_pos['cluster'].unique():\n",
    "        print('modeling cluster {}...'.format(clust))\n",
    "        \n",
    "        # subset data to cluster\n",
    "        train_clust = train_data[train_data['cluster']==clust].copy()\n",
    "        test_clust = test_data[test_data['cluster']==clust].copy()\n",
    "\n",
    "        # subset to positive data\n",
    "        train_clust_pos = train_data_pos[train_data_pos['cluster']==clust].copy()\n",
    "        test_clust_pos = test_data_pos[test_data_pos['cluster']==clust].copy()\n",
    "\n",
    "        train_preds_clust = train_preds[train_preds['cluster']==clust].copy()\n",
    "        test_preds_clust = test_preds[test_preds['cluster']==clust].copy()\n",
    "         \n",
    "        # split features and targets\n",
    "        level_cols = [col for col in train_data.columns if col.startswith('level')]\n",
    "        X_trn = train_clust_pos.drop(labels=['date','time','cluster']+level_cols, axis=1)\n",
    "        Y_trn = train_clust_pos.loc[:,file_args['target_second_stage']].values.ravel()\n",
    "        X_tst = test_clust_pos.drop(labels=['date','time','cluster']+level_cols, axis=1)\n",
    "        Y_tst = test_clust_pos.loc[:,file_args['target_second_stage']].values.ravel()\n",
    "                \n",
    "        model, model_type = util.get_best_model(2, clust)\n",
    "        print('best model for cluster {} is {}'.format(clust, model_type))\n",
    "\n",
    "        if model_type == 'model_avg_baseline':\n",
    "            # join average based predictions to train and test dataframes\n",
    "            train_clust_pos_preds = train_clust_pos.merge(model, how='left', on=['time_idx','segment_id','day_of_week'])\n",
    "            test_clust_pos_preds = test_clust_pos.merge(model, how='left', on=['time_idx','segment_id','day_of_week'])\n",
    "            \n",
    "            # fill null predictions with 0\n",
    "            train_clust_pos_preds[file_args['target_second_stage']+'_preds_avg_baseline'].fillna(value=0, inplace=True)\n",
    "            test_clust_pos_preds[file_args['target_second_stage']+'_preds_avg_baseline'].fillna(value=0, inplace=True)\n",
    "\n",
    "            # get predictions\n",
    "            trn_preds = train_clust_pos_preds[file_args['target_second_stage']+'_preds_avg_baseline'].values\n",
    "            tst_preds = test_clust_pos_preds[file_args['target_second_stage']+'_preds_avg_baseline'].values      \n",
    "                  \n",
    "        else:\n",
    "            trn_preds = model.predict(X_trn)\n",
    "            tst_preds = model.predict(X_tst)\n",
    "        \n",
    "        # create prediction dataframes\n",
    "        train_preds_pos = train_clust_pos[['date_idx','time_idx','segment_id']].copy()\n",
    "        test_preds_pos = test_clust_pos[['date_idx','time_idx','segment_id']].copy()\n",
    "        train_clust_preds = train_clust[['date_idx','time_idx','segment_id',file_args['target_second_stage']]].copy()\n",
    "        test_clust_preds = test_clust[['date_idx','time_idx','segment_id',file_args['target_second_stage']]].copy()\n",
    "\n",
    "        # add positive predictions\n",
    "        train_preds_pos.loc[:,file_args['target_second_stage']+pred_suffix] = trn_preds\n",
    "        test_preds_pos.loc[:,file_args['target_second_stage']+pred_suffix] = tst_preds\n",
    "        \n",
    "        # join to predictions and fillna with 0\n",
    "        train_clust_preds = train_clust_preds.merge(train_preds_pos, how='left', on=['segment_id','date_idx','time_idx'])\n",
    "        test_clust_preds = test_clust_preds.merge(test_preds_pos, how='left', on=['segment_id','date_idx','time_idx'])\n",
    "        train_clust_preds[file_args['target_second_stage']+pred_suffix].fillna(value=0, inplace=True)\n",
    "        test_clust_preds[file_args['target_second_stage']+pred_suffix].fillna(value=0, inplace=True)\n",
    "\n",
    "        # calculate scores for individual cluster\n",
    "        tmp_preds = test_clust_preds[file_args['target_second_stage']+pred_suffix].values\n",
    "        tmp_actuals = test_clust_preds[file_args['target_second_stage']].values\n",
    "        model_test_results[clust] = f1_score(tmp_actuals, tmp_preds, average=file_args['scoring_metric'][3:])\n",
    "        \n",
    "        # drop target column\n",
    "        train_clust_preds.drop(labels=file_args['target_second_stage'], axis=1, inplace=True)\n",
    "        test_clust_preds.drop(labels=file_args['target_second_stage'], axis=1, inplace=True)\n",
    "        \n",
    "        # add results to ensemble dataframe\n",
    "        train_preds_ensemble = train_preds_ensemble.append(train_clust_preds)\n",
    "        test_preds_ensemble = test_preds_ensemble.append(test_clust_preds)\n",
    "\n",
    "    # add prediction columns to dataframes\n",
    "    train_preds = train_preds.merge(train_preds_ensemble, how='left', \n",
    "                                    on=['date_idx','time_idx','segment_id'])\n",
    "    test_preds = test_preds.merge(test_preds_ensemble, how='left', \n",
    "                                  on=['date_idx','time_idx','segment_id'])\n",
    "    # add results to dict\n",
    "    test_results_dict['stage_2']['model_clusters_ensemble'] = model_test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--- second stage predictions took {0:.1f} seconds ---'.format(time.time() - second_modeling_stage_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_results_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add stage 1 cluster counts\n",
    "clust_count_dict = {}\n",
    "for clust in test_data['cluster'].unique():\n",
    "    clust_count_dict[clust] = test_data[test_data['cluster']==clust].shape[0]\n",
    "    \n",
    "test_results_dict['stage_1']['cluster_counts'] = clust_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add stage 2 cluster counts\n",
    "clust_count_dict = {}\n",
    "for clust in test_data_pos['cluster'].unique():\n",
    "    clust_count_dict[clust] = test_data_pos[test_data_pos['cluster']==clust].shape[0]\n",
    "    \n",
    "test_results_dict['stage_2']['cluster_counts'] = clust_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save test_results_dict\n",
    "fn = os.path.join(file_args['save_dir'], 'test_results_dict.pkl')\n",
    "joblib.dump(test_results_dict, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.metrics_plot_model(test_results_dict, stage='stage_1', score_metric=file_args['scoring_metric'], \n",
    "                        sort=True, title_prefix='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.metrics_plot_model(test_results_dict, stage='stage_2', score_metric=file_args['scoring_metric'], \n",
    "                        sort=True, title_prefix='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--- evaluating results took {0:.1f} seconds ---'.format(time.time() - eval_results_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('--- step 3 took {0:.1f} seconds ---'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. write predictions to csv and database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get info from database\n",
    "sqlalchemy_conn_str_file = open(sqlalchemy_conn_str_file, 'r').read()\n",
    "engine = create_engine(sqlalchemy_conn_str_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_preds.to_sql('train_predictions', con=engine, if_exists='replace', chunksize=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_preds.to_sql('test_predictions', con=engine, if_exists='replace', chunksize=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fn = os.path.join(save_dir, 'train_predictions.csv')\n",
    "train_preds.to_csv(fn, index=False, chunksize=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fn = os.path.join(save_dir, 'test_predictions.csv')\n",
    "test_preds.to_csv(fn, index=False, chunksize=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
