{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSE220 Homework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers Section\n",
    "##### See below for corresponding code\n",
    "*Question 1:*  Compute and report the prior probabilities $\\pi_j$ for all labels.\n",
    "\n",
    "<pre>\n",
    "class prior probabilities:\n",
    "0    0.098667\n",
    "1    0.111833\n",
    "2    0.096833\n",
    "3    0.101333\n",
    "4    0.103833\n",
    "5    0.085667\n",
    "6    0.101333\n",
    "7    0.108500\n",
    "8    0.091833\n",
    "9    0.100167\n",
    "Name: priors, dtype: float64\n",
    "</pre>\n",
    "\n",
    "*Question 2:*  For each pixel $X_i$ and label j, compute $P_{ji} = P(X_i = 1 \\mid y = j)$ (Use the maximum likelihood estimate shown in class). Use Laplacian Smoothing for computing $P_{ji}$. Report the highest $P_{ji}$ for each label j.\n",
    "\n",
    "<pre>\n",
    "0    0.851852\n",
    "1    0.985141\n",
    "2    0.728988\n",
    "3    0.808197\n",
    "4    0.849600\n",
    "5    0.711240\n",
    "6    0.849180\n",
    "7    0.794793\n",
    "8    0.875226\n",
    "9    0.867330\n",
    "dtype: float64\n",
    "</pre>\n",
    "\n",
    "*Question 3:*  Use naive bayes (as shown in lecture slides) to classify the test data. Report the accuracy.\n",
    "\n",
    "<pre>\n",
    "Test accuracy = 0.809\n",
    "</pre>\n",
    "\n",
    "*Question 4:*  Compute the confusion matrix (as shown in the lectures) and report the top 3 pairs with most (absolute number) incorrect classifications.\n",
    "\n",
    "<pre>\n",
    "Confusion Matrix:\n",
    " [[ 74   0   0   0   0   5   2   0   4   0]\n",
    " [  0 120   0   0   0   4   1   0   1   0]\n",
    " [  1   7  88   4   0   1   2   3   8   2]\n",
    " [  0   2   1  86   1   6   3   2   3   3]\n",
    " [  1   1   1   0  83   0   2   0   1  21]\n",
    " [  3   1   1  11   2  62   2   3   1   1]\n",
    " [  3   0   4   0   3   4  73   0   0   0]\n",
    " [  0   6   2   0   3   1   0  77   3   7]\n",
    " [  0   2   2   9   4   3   1   2  61   5]\n",
    " [  0   1   0   1   4   0   0   0   3  85]]\n",
    " \n",
    "21 times the digit 4 was incorrectly classified as 9\n",
    "11 times the digit 5 was incorrectly classified as 3\n",
    "9 times the digit 8 was incorrectly classified as 3\n",
    "</pre>\n",
    "\n",
    "*Question 5:*  Visualizing mistakes: Print two MNIST images from the test data that your classifier misclassified. Write both the true and predicted labels for both of these misclassified digits.\n",
    "\n",
    "<pre>\n",
    "(see code below for misclassified digit images)\n",
    "</pre>\n",
    "\n",
    "*Question 6:*  Implement Gaussian Mixture model on the data as shown in class. Tune the covariance type parameter on the validation data. Use the selected value to compute the test accuracy. As always, train the model on train+validation data to compute the test accuracy.\n",
    "\n",
    "<pre>\n",
    "Test accuracy = 0.938053097345\n",
    "</pre>\n",
    "\n",
    "*Question 7:*  Apply Linear Discriminant Analysis model on the train+validation data and report the accuracy obtained on test data. Report the transformation matrix (w) along with the intercept.\n",
    "\n",
    "<pre>\n",
    "coefficients:\n",
    " [  5.99249063e+00  -2.30925547e-01  -7.16677835e-01  -8.33967114e-03\n",
    "  -5.43714783e+01   7.92096908e+01  -3.44319554e+00  -3.32058792e+01\n",
    "   1.84143157e+01  -4.92407304e+01  -9.26564180e+00  -2.65341746e-01\n",
    "   6.24717559e-01   1.18597946e-02  -2.90874696e+02  -9.03149024e-01\n",
    "   4.32297783e+01  -1.62317393e+02   1.66223179e+01   1.93288295e+02\n",
    "  -3.55515669e+00  -6.74413495e-02  -4.01932777e-02   2.36237508e-02\n",
    "   1.65662819e+01   3.55893095e-01  -6.66649647e+00  -1.76547404e+01\n",
    "  -1.69654197e+01  -7.84606620e+01]\n",
    "  \n",
    "intercept:\n",
    " [ 50.7607485]\n",
    "</pre>\n",
    "\n",
    "*Question 8:*  Load the digits dataset (scikit-learnâ€™s toy dataset) and take the last 1300 samples as your test set. Train a K-Nearest Neighbor (k=5, linf distance) model and then without using any scikit-learn method, report the final values for Specificity, Sensitivity, TPR, TNR, FNR, FPR, Precision and Recall for Digit 3 (this digit is a positive, everything else is a negative).\n",
    "\n",
    "<pre>\n",
    "TPR = sensitivity = recall = TP / (TP + FN) = 0.869\n",
    "TNR = specificity = TN / (FP + TN) = 0.989\n",
    "FPR = 1 - specificity = FP / (FP + TN) = 0.011\n",
    "FNR = 1 - sensitivity = FN / (FN + TP) = 0.131\n",
    "precision = TP / (TP + FP) = 0.897\n",
    "\n",
    "Note:\n",
    "sensitivity = recall = TPR \n",
    "specificity = TNR\n",
    "FPR = 1 - specificity\n",
    "FNR = 1 - sensitivity\n",
    "\n",
    "TP: 113\n",
    "TN: 1157\n",
    "FP: 13\n",
    "FN: 17\n",
    "</pre>\n",
    "\n",
    "*Question 9:*  Perform least squares regression on this dataset. Report the mean squared error and the mean absolute error on the test data.\n",
    "\n",
    "<pre>\n",
    "MSE test = 2155.96465\n",
    "MAE test = 36.31813\n",
    "</pre>\n",
    "\n",
    "*Question 10:*  Repeat the experiment from Question 10 for all possible values of ablation (i.e., removing the feature 1 only, then removing the feature 2 only, and so on). Report all MSEs.\n",
    "\n",
    "<pre>\n",
    "MSE for model without feature 0 = 2152.80664\n",
    "MSE for model without feature 1 = 2259.13308\n",
    "MSE for model without feature 2 = 2783.51448\n",
    "MSE for model without feature 3 = 2424.77235\n",
    "MSE for model without feature 4 = 2187.59952\n",
    "MSE for model without feature 5 = 2167.51761\n",
    "MSE for model without feature 6 = 2159.15148\n",
    "MSE for model without feature 7 = 2153.06317\n",
    "MSE for model without feature 8 = 2335.17338\n",
    "MSE for model without feature 9 = 2165.86619\n",
    "</pre>\n",
    "\n",
    "*Question 11:*  Based on the MSE values obtained from Question 11, which features do you deem the most/least significant and why?\n",
    "\n",
    "<pre>\n",
    "Removal of significant features should result in a worse model, with a higher MSE. So the most significant features are those that result in the highest MSE when removed. Conversely, the least significant features should be those that, when removed, result in the lowest MSE.\n",
    "\n",
    "5 Most Significant Features: [2 3 8 1 4]\n",
    "MSE for model without feature 2 = 2783.51448185\n",
    "MSE for model without feature 3 = 2424.772348\n",
    "MSE for model without feature 8 = 2335.17338461\n",
    "MSE for model without feature 1 = 2259.13307937\n",
    "MSE for model without feature 4 = 2187.59951938\n",
    "\n",
    "5 Least Significant Features: [0 7 6 9 5]\n",
    "MSE for model without feature 0 = 2152.80664218\n",
    "MSE for model without feature 7 = 2153.06317113\n",
    "MSE for model without feature 6 = 2159.15148251\n",
    "MSE for model without feature 9 = 2165.86619219\n",
    "MSE for model without feature 5 = 2167.51760615\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read csv files\n",
    "x_train = pd.read_csv(\"mnist_train_data.csv\", header=None)\n",
    "y_train = pd.read_csv(\"mnist_train_labels.csv\", header=None, names=['label'])\n",
    "x_test = pd.read_csv(\"mnist_test_data.csv\", header=None)\n",
    "y_test = pd.read_csv(\"mnist_test_labels.csv\", header=None, names=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (6000, 784)\n",
      "y_train shape: (6000, 1)\n",
      "x_test shape: (1000, 784)\n",
      "y_test shape: (1000, 1)\n"
     ]
    }
   ],
   "source": [
    "# check shape of data\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: ##\n",
    "Compute and report the prior probabilities $\\pi_j$ for all labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get list of distinct label values\n",
    "labels = np.unique(y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class prior probabilities:\n",
      "\n",
      "0    0.098667\n",
      "1    0.111833\n",
      "2    0.096833\n",
      "3    0.101333\n",
      "4    0.103833\n",
      "5    0.085667\n",
      "6    0.101333\n",
      "7    0.108500\n",
      "8    0.091833\n",
      "9    0.100167\n",
      "Name: priors, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# calculate prior probabilities of labels from training data\n",
    "priors = []\n",
    "total_rows = y_train.shape[0]\n",
    "\n",
    "# via for loop\n",
    "for i in range(10):\n",
    "    count_i = np.sum(y_train.label == i)\n",
    "    frac_i = count_i / total_rows\n",
    "    priors.append(frac_i)\n",
    "    \n",
    "# using pandas\n",
    "occurrences = y_train.label.value_counts(normalize=False).sort_index().rename('occurrences')\n",
    "priors = y_train.label.value_counts(normalize=True).sort_index().rename('priors')\n",
    "\n",
    "print('class prior probabilities:\\n')\n",
    "print(priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: ##\n",
    "For each pixel $X_i$ and label j, compute $P_{ji} = P(X_i = 1 \\mid y = j)$ (Use the maximum likelihood estimate shown in class). Use Laplacian Smoothing for computing $P_{ji}$. Report the highest $P_{ji}$ for each label j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add labels to train data\n",
    "train_labeled = pd.concat([y_train, x_train], axis=1)\n",
    "\n",
    "# group by label and sum\n",
    "train_class_pixel_sums = train_labeled.groupby(by=['label']).sum().reset_index().drop(['label'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add one to each sum\n",
    "train_class_pixel_sums_smoothed = train_class_pixel_sums + 1\n",
    "\n",
    "# adjust occurrences by adding number possible pixel outcomes\n",
    "occurrences_smoothed = occurrences + 2\n",
    "\n",
    "# divide class pixel counts + 1 by instances of each class + 2\n",
    "pixel_probs = train_class_pixel_sums_smoothed.divide(occurrences_smoothed, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEV5JREFUeJzt3VuM3dV1x/HfwjY2tvFlfBlsY+MLpoCN6kgjVCmoSpUm\nIiiSyQsKD5EroTgPadRIeSiiD+URVU0iHqpITrFiqpSkUoLgAbUCVAlFQhEDItiODTbD+Dr2jO8e\nY3xj9WH+0AHmrDWc+8z+fiTLM2ed/5w9f8/P58xZe++/ubsAlOemTg8AQGcQfqBQhB8oFOEHCkX4\ngUIRfqBQhB8oFOEHCkX4gULNbOeDLV261NeuXdvOhwSKMjg4qFOnTtlk7ttQ+M3sQUlPS5oh6d/d\n/ano/mvXrlV/f38jDwkg0NfXN+n71v2y38xmSPo3Sd+SdK+kR83s3nq/HoD2auR3/vslHXT3AXe/\nKuk3krY2Z1gAWq2R8K+SdGTc50er2z7DzLabWb+Z9Y+MjDTwcACaqeXv9rv7Dnfvc/e+ZcuWtfrh\nAExSI+E/Jmn1uM9vr24DMAU0Ev43JG00s3VmdrOk70p6sTnDAtBqdbf63P26mf29pP/RWKtvp7vv\nbdrIALRUQ31+d39J0ktNGguANmJ6L1Aowg8UivADhSL8QKEIP1Aowg8UivADhSL8QKEIP1Aowg8U\nivADhSL8QKEIP1Cotm7djYm5e0P1yPXr1+s+djKPbRbvEn3TTfU/vzT6tbPjS8czP1Aowg8UivAD\nhSL8QKEIP1Aowg8UivADhaLPX8n62R9//HHN2o0bN8Jjr169GtavXLkS1oeHh8P6qVOnatbOnTsX\nHnvx4sWwHn3fkzF37tyatcWLF4fH9vT0hPXsClALFy6sWZszZ054bAlzCHjmBwpF+IFCEX6gUIQf\nKBThBwpF+IFCEX6gUA31+c1sUNJFSTckXXf3vmYMqh6Nrom/du1aWI968efPnw+PPXv2bFg/dOhQ\nWH/33XfD+oEDB2rWBgYGwmNPnz4d1rM5CjNnxj9Cy5cvr1lbv359eOy6devC+oYNG8L63XffXbO2\ncuXK8NhojoA0PeYBNGOSz9+4e+1ZJgC6Ei/7gUI1Gn6X9IqZvWlm25sxIADt0ejL/gfc/ZiZLZf0\nspntd/fXxt+h+k9huyStWbOmwYcD0CwNPfO7+7Hq72FJz0u6f4L77HD3PnfvyxZiAGifusNvZvPM\n7NZPPpb0TUl7mjUwAK3VyMv+XknPVy2NmZL+093/uymjAtBydYff3Qck/WUTx9JS2f712Zr6aF38\n4OBgeOzhw4fD+sGDB8P6/v37w3r0+CdOnAiPvXDhQljP+vzZXgbRHIZsn4JsDkI2vyKa25H16bP5\nC/PmzQvrM2bMCOvdgFYfUCjCDxSK8AOFIvxAoQg/UCjCDxRq2mzd3cjW21Le6ouW/Gbtrmy5cDa2\nRYsWhfWNGzfWrGXbX2ctr2zsly5dqruetV+zbcezNubx48dr1lavXh0em31ft9xyS1in1QegaxF+\noFCEHygU4QcKRfiBQhF+oFCEHyhUMX3+RutR33b+/Pnhsdn2ZQsWLAjr2dbfkayPn9Wzx86W5R45\ncqRmLVtOnM1/yOZXRLI5Bpns52Uq4JkfKBThBwpF+IFCEX6gUIQfKBThBwpF+IFCTZs+f3ZJ5Gwr\n5mx9dvT1Z82aFR67ePHisN7b2xvWs57y5cuXa9aysWVbc2eXD8+2z44eP1szn81/uPXWW8N6tL12\ntvV2dt6mwnr9DM/8QKEIP1Aowg8UivADhSL8QKEIP1Aowg8UKu3zm9lOSd+WNOzum6vbeiT9VtJa\nSYOSHnH3+hedN0HW58/6slmfP5onkK0Nz752tm492zs/uuZAth4/2tteyi8fvnv37rB+9OjRsB5Z\nvnx5WM/23r/99ttr1rK5F9keDdk+CFPBZL6DX0l68HO3PS7pVXffKOnV6nMAU0gafnd/TdKZz928\nVdKu6uNdkh5u8rgAtFi9r1163X2o+viEpHh+KoCu0/AvLj428bzm5HMz225m/WbWPzIy0ujDAWiS\nesN/0sxWSFL1d81dHN19h7v3uXvfsmXL6nw4AM1Wb/hflLSt+nibpBeaMxwA7ZKG38yek/S6pL8w\ns6Nm9pikpyR9w8wOSPrb6nMAU0ja53f3R2uUvt7ksTQk6/Nnsr7tzTffXPex2RyD0dHRsH7q1Kmw\nHvXq33///fDY/v7+sL537966H1uKz82GDRvCY6M+vSStWrWq7uN7enrCY7P9Hxr9eesGU3+mAoC6\nEH6gUIQfKBThBwpF+IFCEX6gUNNm6+5MJ1sz0ZJbSTp58mRY379/f1h//fXXa9ayVl1WHxoaCuuZ\naNlttvX2kiVLGqpHX7/RS5dPB9P/OwQwIcIPFIrwA4Ui/EChCD9QKMIPFIrwA4Uqps/fStnW3Rcu\nXAjrWS896/MPDAzUrO3bty88NlsunG0rnm1LHp2bbP5DVv/oo4/C+sWLF2vWsq27S8AzP1Aowg8U\nivADhSL8QKEIP1Aowg8UivADhaLPP0ljVyWbWNYLv3r1ali/dOlSWM/2Ipg9e3bN2ooVK8JjszXx\n2WPfuHEjrEdbnmfHHj58OKwvXLgwrEdXiMq27p4zZ05YnzVrVlifClt788wPFIrwA4Ui/EChCD9Q\nKMIPFIrwA4Ui/ECh0j6/me2U9G1Jw+6+ubrtSUnflzRS3e0Jd3+pVYNshqhPL+W9+uj4rOe7aNGi\nsJ5dajrrhy9YsKBmbfPmzeGx165dC+vZ93b58uWwHvXqs30Msr0GBgcHw3p0XpcuXRoeO3fu3LAe\nnXNp+vT5fyXpwQlu/7m7b6n+dHXwAXxRGn53f03SmTaMBUAbNfI7/4/M7B0z22lm7IkETDH1hv8X\nktZL2iJpSNJPa93RzLabWb+Z9Y+MjNS6G4A2qyv87n7S3W+4+8eSfinp/uC+O9y9z937ooUWANqr\nrvCb2filYt+RtKc5wwHQLpNp9T0n6WuSlprZUUn/LOlrZrZFkksalPSDFo4RQAuk4Xf3Rye4+ZkW\njCXVyJr6rJ6Jeu3Z147WtEvSHXfcEdaznvSmTZtq1rK977M+/ejoaFg/cyZuBM2fP79m7cMPPwyP\nPX/+fFjP5gFE10uI9vSX4r0ApPzf/Kabun/+XPePEEBLEH6gUIQfKBThBwpF+IFCEX6gUNNm6+5s\nyW62fXZ2uedoK+dG24jZ8tGs3soWaNbqO3jwYFg/fvx4zVrUBpzMY2djjy4Pnm2XnrUhsyW9UwHP\n/EChCD9QKMIPFIrwA4Ui/EChCD9QKMIPFGra9PmzpanZ8tBsHkC0PDTbpjm73HN0iW0pXx46c2bt\nf8bs2KxXnn1v2XmLls5mczOiPr2UbysefW8zZswIjy0Bz/xAoQg/UCjCDxSK8AOFIvxAoQg/UCjC\nDxRqSvX5o75w1m8+ffp0WD9x4kTdjx312aV8Pf5tt90W1rOvH/W7sz59tm790KFDYT27THa0nj+7\nfFu2Zn7JkiVhPZpfkW2nns0hmA545gcKRfiBQhF+oFCEHygU4QcKRfiBQhF+oFBpn9/MVkt6VlKv\nJJe0w92fNrMeSb+VtFbSoKRH3P1s64Yar8/O1qVn+/KfO3curJ88ebJmLZtjkPX5ly9fHtYXL14c\n1qOedbY/fTa/YWBgIKzv2bMnrA8NDdWsZfscZOclmwewaNGimrXs3ySbB5DNn5gKJvPMf13ST9z9\nXkl/JemHZnavpMclveruGyW9Wn0OYIpIw+/uQ+7+VvXxRUn7JK2StFXSrupuuyQ93KpBAmi+L/U7\nv5mtlfQVSX+U1Ovun7ymO6GxXwsATBGTDr+ZzZf0O0k/dvfPbGjnYxPfJ5z8bmbbzazfzPqzudwA\n2mdS4TezWRoL/q/d/ffVzSfNbEVVXyFpeKJj3X2Hu/e5e9+yZcuaMWYATZCG38be1nxG0j53/9m4\n0ouStlUfb5P0QvOHB6BVJrOk96uSvidpt5m9Xd32hKSnJP2XmT0m6ZCkR1ozxP8XtVeyrZiz1k22\nxXW0dXd2meorV66E9UYuDy7F5yX72tmW5sPDE76g+1S2JHjevHk1a7298dtE2VLndevWhfVVq1bV\nrEVtQCn/eZkOW3+n4Xf3P0iq9dP19eYOB0C7MMMPKBThBwpF+IFCEX6gUIQfKBThBwo1pbbujnrx\n2RLNbHZhtvT17Nnaq5Wj7akl6ciRI2E9Wi4sxXMMpMYug53NQWh02e369etr1u67777w2HvuuSes\n33nnnWF9xYoVNWvZMulG5lZMFTzzA4Ui/EChCD9QKMIPFIrwA4Ui/EChCD9QqCnV5496q9kllbO+\nbta3jb7+/Pnzw2N7enrC+nvvvRfWP/jgg7A+e/bsmrVsS/Ns++s1a9aE9U2bNoX1qM9/1113hcdG\nfXopP6/RXgLZevzpsF4/wzM/UCjCDxSK8AOFIvxAoQg/UCjCDxSK8AOFmlJ9/ki27342DyDbx72R\n/ec3btwY1rM19aOjo2E92osgm78wc2b8I5DNYcj21o/Oa/a1o/kLUj72SPbzMh3W62d45gcKRfiB\nQhF+oFCEHygU4QcKRfiBQhF+oFBpo9TMVkt6VlKvJJe0w92fNrMnJX1f0kh11yfc/aVWDTTTyHr8\nydSjdfFZv3rlypVhPdtbv5F6dmyj8wCy46N+etZrz8aeKaFX34jJzJK4Lukn7v6Wmd0q6U0ze7mq\n/dzd/7V1wwPQKmn43X1I0lD18UUz2ydpVasHBqC1vtTv/Ga2VtJXJP2xuulHZvaOme00swn3yTKz\n7WbWb2b9IyMjE90FQAdMOvxmNl/S7yT92N0vSPqFpPWStmjslcFPJzrO3Xe4e5+792XXywPQPpMK\nv5nN0ljwf+3uv5ckdz/p7jfc/WNJv5R0f+uGCaDZ0vDb2Fumz0ja5+4/G3f7+K1VvyNpT/OHB6BV\nJvNu/1clfU/SbjN7u7rtCUmPmtkWjbX/BiX9oCUjbJJG2z7dvJVzoy2xRrSynUarrrUm827/HyRN\n9K/QsZ4+gMYxww8oFOEHCkX4gUIRfqBQhB8oFOEHCjVttu4uGf1w1INnfqBQhB8oFOEHCkX4gUIR\nfqBQhB8oFOEHCmXtXAtuZiOSDo27aamkU20bwJfTrWPr1nFJjK1ezRzbHe4+qf3y2hr+Lzy4Wb+7\n93VsAIFuHVu3jktibPXq1Nh42Q8UivADhep0+Hd0+PEj3Tq2bh2XxNjq1ZGxdfR3fgCd0+lnfgAd\n0pHwm9mDZvaumR00s8c7MYZazGzQzHab2dtm1t/hsew0s2Ez2zPuth4ze9nMDlR/T3iZtA6N7Ukz\nO1adu7fN7KEOjW21mf2vmf3ZzPaa2T9Ut3f03AXj6sh5a/vLfjObIek9Sd+QdFTSG5Iedfc/t3Ug\nNZjZoKQ+d+94T9jM/lrSqKRn3X1zddu/SDrj7k9V/3Eudvd/7JKxPSlptNNXbq4uKLNi/JWlJT0s\n6e/UwXMXjOsRdeC8deKZ/35JB919wN2vSvqNpK0dGEfXc/fXJJ353M1bJe2qPt6lsR+etqsxtq7g\n7kPu/lb18UVJn1xZuqPnLhhXR3Qi/KskHRn3+VF11yW/XdIrZvammW3v9GAm0FtdNl2STkjq7eRg\nJpBeubmdPndl6a45d/Vc8brZeMPvix5w9y2SviXph9XL267kY7+zdVO7ZlJXbm6XCa4s/alOnrt6\nr3jdbJ0I/zFJq8d9fnt1W1dw92PV38OSnlf3XX345CcXSa3+Hu7weD7VTVdunujK0uqCc9dNV7zu\nRPjfkLTRzNaZ2c2SvivpxQ6M4wvMbF71RozMbJ6kb6r7rj78oqRt1cfbJL3QwbF8RrdcubnWlaXV\n4XPXdVe8dve2/5H0kMbe8X9f0j91Ygw1xrVe0p+qP3s7PTZJz2nsZeA1jb038pikJZJelXRA0iuS\nerpobP8habekdzQWtBUdGtsDGntJ/46kt6s/D3X63AXj6sh5Y4YfUCje8AMKRfiBQhF+oFCEHygU\n4QcKRfiBQhF+oFCEHyjU/wFJqP8dUBSCUgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11890ba90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check that pixel probs returns something sensible for a given digit/class\n",
    "img_mean = pixel_probs.iloc[3,:]\n",
    "plt.imshow(img_mean.values.reshape((28,28)),cmap=plt.cm.gray_r);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.851852\n",
       "1    0.985141\n",
       "2    0.728988\n",
       "3    0.808197\n",
       "4    0.849600\n",
       "5    0.711240\n",
       "6    0.849180\n",
       "7    0.794793\n",
       "8    0.875226\n",
       "9    0.867330\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the highest P_ij for each label j\n",
    "pixel_probs.max(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: ##\n",
    "Use naive bayes (as shown in lecture slides) to classify the test data. Report the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate log prior probabilities (and reshape so we can add to log likelihoods)\n",
    "log_priors = np.log(priors).values.reshape((10,1))\n",
    "\n",
    "# calculate log likelihoods\n",
    "log_likelihoods_train = np.dot(np.log(pixel_probs),x_train.T) + np.dot(np.log(1-pixel_probs),(1-x_train).T)\n",
    "log_likelihoods_test = np.dot(np.log(pixel_probs),x_test.T) + np.dot(np.log(1-pixel_probs),(1-x_test).T)\n",
    "\n",
    "# calculate posterior probabilities for each class\n",
    "class_probs_train = log_priors + log_likelihoods_train\n",
    "class_probs_test = log_priors + log_likelihoods_test\n",
    "\n",
    "# make predictions based on posterior probabilities\n",
    "train_pred = np.argmax(class_probs_train, axis=0)\n",
    "test_pred = np.argmax(class_probs_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy = 0.845166666667\n",
      "Test accuracy = 0.809\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# print the accuracy\n",
    "print ('Training accuracy = ' + str(accuracy_score(train_pred, y_train)))\n",
    "print ('Test accuracy = ' + str(accuracy_score(test_pred, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: ##\n",
    "Compute the confusion matrix (as shown in the lectures) and report the top 3 pairs with most (absolute number) incorrect classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[ 74   0   0   0   0   5   2   0   4   0]\n",
      " [  0 120   0   0   0   4   1   0   1   0]\n",
      " [  1   7  88   4   0   1   2   3   8   2]\n",
      " [  0   2   1  86   1   6   3   2   3   3]\n",
      " [  1   1   1   0  83   0   2   0   1  21]\n",
      " [  3   1   1  11   2  62   2   3   1   1]\n",
      " [  3   0   4   0   3   4  73   0   0   0]\n",
      " [  0   6   2   0   3   1   0  77   3   7]\n",
      " [  0   2   2   9   4   3   1   2  61   5]\n",
      " [  0   1   0   1   4   0   0   0   3  85]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test, test_pred)\n",
    "\n",
    "print('Confusion Matrix:\\n',cnf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 times the digit 4 was incorrectly classified as 9\n",
      "11 times the digit 5 was incorrectly classified as 3\n",
      "9 times the digit 8 was incorrectly classified as 3\n"
     ]
    }
   ],
   "source": [
    "# set diagonal values to 0\n",
    "np.fill_diagonal(cnf_matrix, 0)\n",
    "\n",
    "# find top 3 values\n",
    "flattened = cnf_matrix.ravel()\n",
    "unique = np.unique(flattened)\n",
    "sorted = np.sort(unique)\n",
    "top3 = sorted[-3:]\n",
    "\n",
    "# find where top3 values occur\n",
    "top_misclassified = np.dstack(np.where(cnf_matrix >= np.min(top3)))[0]\n",
    "\n",
    "# print results\n",
    "for pair in top_misclassified:\n",
    "    print(str(cnf_matrix[pair[0],pair[1]]), 'times the digit',\n",
    "          str(pair[0]),'was incorrectly classified as',str(pair[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: ## \n",
    "Visualizing mistakes: Print two MNIST images from the test data that your classifier misclassified. Write both the true and predicted labels for both of these misclassified digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 282 is 7 but was misclassified as 1 :\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAACvxJREFUeJzt3U+InPUdx/HPp/65qIekmS5LjF2FUAiFRhhCQSkWq8Rc\nohcxB0lBWA+2KHio2EM9hlKVHoqw1mBarFJQMYfQEoMQhCKOkuaPaRsrK2aJ2Qk5GE82+u1hH2WM\nuzvjPM8zz7P5vl8wzMyzs5lvhrzzzMwzuz9HhADk852mBwDQDOIHkiJ+ICniB5IifiAp4geSIn4g\nKeIHkiJ+IKkrJ3lnGzZsiJmZmUneJZDK/Py8zp0751FuWyp+29sl/V7SFZL+GBF7Vrv9zMyMer1e\nmbsEsIputzvybcd+2m/7Ckl/kHSXpC2SdtneMu6fB2Cyyrzm3ybp/Yj4ICI+k/SSpJ3VjAWgbmXi\n3yjpo4Hrp4ttX2N71nbPdq/f75e4OwBVqv3d/oiYi4huRHQ7nU7ddwdgRGXiX5C0aeD69cU2AGtA\nmfjflrTZ9o22r5Z0n6T91YwFoG5jH+qLiIu2fyHp71o61Lc3Ik5UNhmAWpU6zh8RByQdqGgWABPE\nx3uBpIgfSIr4gaSIH0iK+IGkiB9IiviBpIgfSIr4gaSIH0iK+IGkiB9IiviBpIgfSIr4gaSIH0iK\n+IGkiB9IiviBpIgfSIr4gaSIH0iK+IGkiB9IiviBpIgfSIr4gaSIH0iK+IGkSq3Sa3te0gVJn0u6\nGBHdKobC5cP22N8bERVOgkuVir/w04g4V8GfA2CCeNoPJFU2/pD0uu13bM9WMRCAySj7tP/WiFiw\n/T1JB23/KyIOD96g+E9hVpJuuOGGkncHoCql9vwRsVCcL0p6VdK2ZW4zFxHdiOh2Op0ydwegQmPH\nb/sa29d9eVnSnZKOVzUYgHqVedo/JenV4lDOlZL+EhF/q2QqALUbO/6I+EDSjyqcBQ0ocxy+bsNm\n43MA5XCoD0iK+IGkiB9IiviBpIgfSIr4gaSq+Kk+NKzNh+vK4FBevdjzA0kRP5AU8QNJET+QFPED\nSRE/kBTxA0lxnH8NaPI4ftlj7WVm50d668WeH0iK+IGkiB9IiviBpIgfSIr4gaSIH0iK4/wtUOdx\n/LV8LHwtz74WsOcHkiJ+ICniB5IifiAp4geSIn4gKeIHkhoav+29thdtHx/Ytt72QdunivN19Y55\neYuI2k51s73qCe01yp7/eUnbL9n2mKRDEbFZ0qHiOoA1ZGj8EXFY0vlLNu+UtK+4vE/S3RXPBaBm\n477mn4qIM8XljyVNVTQPgAkp/YZfLL2wXPHFpe1Z2z3bvX6/X/buAFRk3PjP2p6WpOJ8caUbRsRc\nRHQjotvpdMa8OwBVGzf+/ZJ2F5d3S3qtmnEATMooh/pelPQPST+wfdr2A5L2SLrD9ilJPyuuA1hD\nhv48f0TsWuFLt1c8C4AJ4hN+QFLEDyRF/EBSxA8kRfxAUsQPJMWv7kZj+NXczWLPDyRF/EBSxA8k\nRfxAUsQPJEX8QFLEDyRF/EBSxA8kRfxAUsQPJEX8QFLEDyRF/EBSxA8kRfxAUsQPJEX8QFLEDyRF\n/EBSxA8kRfxAUsQPJDU0ftt7bS/aPj6w7QnbC7aPFKcd9Y4JoGqj7Pmfl7R9me1PR8TW4nSg2rEA\n1G1o/BFxWNL5CcwCYILKvOb/pe2jxcuCdZVNBGAixo3/GUk3Sdoq6YykJ1e6oe1Z2z3bvX6/P+bd\nAajaWPFHxNmI+DwivpD0rKRtq9x2LiK6EdHtdDrjzgmgYmPFb3t64Oo9ko6vdFsA7TR0iW7bL0q6\nTdIG26cl/UbSbba3SgpJ85IerHFGADUYGn9E7Fpm83M1zIIWst30CKgJn/ADkiJ+ICniB5IifiAp\n4geSIn4gqaGH+nB541BeXuz5gaSIH0iK+IGkiB9IiviBpIgfSIr4gaQ4zr8GXK7H4of9vSJiQpPk\nxJ4fSIr4gaSIH0iK+IGkiB9IiviBpIgfSIrj/GgMx/GbxZ4fSIr4gaSIH0iK+IGkiB9IiviBpIgf\nSGpo/LY32X7D9nu2T9h+uNi+3vZB26eK83X1j5tTRDR2qnN2NGuUPf9FSY9GxBZJP5b0kO0tkh6T\ndCgiNks6VFwHsEYMjT8izkTEu8XlC5JOStooaaekfcXN9km6u64hAVTvW73mtz0j6WZJb0maiogz\nxZc+ljRV6WQAajVy/LavlfSypEci4pPBr8XSC7hlX8TZnrXds93r9/ulhgVQnZHit32VlsJ/ISJe\nKTaftT1dfH1a0uJy3xsRcxHRjYhup9OpYmYAFRjl3X5Lek7SyYh4auBL+yXtLi7vlvRa9eMBqMso\nP9J7i6T7JR2zfaTY9rikPZL+avsBSR9KureeEVGny/XXgmO4ofFHxJuSVvoXcnu14wCYFD7hByRF\n/EBSxA8kRfxAUsQPJEX8QFLEDyRF/EBSxA8kRfxAUsQPJEX8QFLEDyRF/EBSxA8kRfxAUsQPJEX8\nQFLEDyRF/EBSxA8kRfxAUqP83n5gRSy1vXax5weSIn4gKeIHkiJ+ICniB5IifiAp4geSGhq/7U22\n37D9nu0Tth8utj9he8H2keK0o/5xUbWIKHXC2jXKh3wuSno0It61fZ2kd2wfLL72dET8rr7xANRl\naPwRcUbSmeLyBdsnJW2sezAA9fpWr/ltz0i6WdJbxaZf2j5qe6/tdSt8z6ztnu1ev98vNSyA6owc\nv+1rJb0s6ZGI+ETSM5JukrRVS88Mnlzu+yJiLiK6EdHtdDoVjAygCiPFb/sqLYX/QkS8IkkRcTYi\nPo+ILyQ9K2lbfWMCqNoo7/Zb0nOSTkbEUwPbpwdudo+k49WPB6Auo7zbf4uk+yUds32k2Pa4pF22\nt0oKSfOSHqxlQgC1GOXd/jcleZkvHah+HACTwif8gKSIH0iK+IGkiB9IiviBpIgfSIr4gaSIH0iK\n+IGkiB9IiviBpIgfSIr4gaSIH0jKk/z1y7b7kj4c2LRB0rmJDfDttHW2ts4lMdu4qpzt+xEx0u/L\nm2j837hzuxcR3cYGWEVbZ2vrXBKzjaup2XjaDyRF/EBSTcc/1/D9r6ats7V1LonZxtXIbI2+5gfQ\nnKb3/AAa0kj8trfb/rft920/1sQMK7E9b/tYsfJwr+FZ9tpetH18YNt62wdtnyrOl10mraHZWrFy\n8yorSzf62LVtxeuJP+23fYWk/0i6Q9JpSW9L2hUR7010kBXYnpfUjYjGjwnb/omkTyX9KSJ+WGz7\nraTzEbGn+I9zXUT8qiWzPSHp06ZXbi4WlJkeXFla0t2Sfq4GH7tV5rpXDTxuTez5t0l6PyI+iIjP\nJL0kaWcDc7ReRByWdP6SzTsl7Ssu79PSP56JW2G2VoiIMxHxbnH5gqQvV5Zu9LFbZa5GNBH/Rkkf\nDVw/rXYt+R2SXrf9ju3ZpodZxlSxbLokfSxpqslhljF05eZJumRl6dY8duOseF013vD7plsjYquk\nuyQ9VDy9baVYes3WpsM1I63cPCnLrCz9lSYfu3FXvK5aE/EvSNo0cP36YlsrRMRCcb4o6VW1b/Xh\ns18uklqcLzY8z1fatHLzcitLqwWPXZtWvG4i/rclbbZ9o+2rJd0naX8Dc3yD7WuKN2Jk+xpJd6p9\nqw/vl7S7uLxb0msNzvI1bVm5eaWVpdXwY9e6Fa8jYuInSTu09I7/fyX9uokZVpjrJkn/LE4nmp5N\n0otaehr4Py29N/KApO9KOiTplKTXJa1v0Wx/lnRM0lEthTbd0Gy3aukp/VFJR4rTjqYfu1XmauRx\n4xN+QFK84QckRfxAUsQPJEX8QFLEDyRF/EBSxA8kRfxAUv8HGwJeE2bHeBMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1272fdba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 939 is 2 but was misclassified as 0 :\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAACz9JREFUeJzt3U+oXPUZxvHnqX826iJppiHE2KsQCqHQCEMoKMVilZhN\ndCNmISkI14UVBRcVu6jLUKrSRRGuNZgWqxRUzCK0xCAEoYijpPlj2kblirnE3AlZGFc2+nZxjzIm\n986MM+fMOXPf7wcOc+Y35+a89yRPzp/fOfNzRAhAPt+ruwAA9SD8QFKEH0iK8ANJEX4gKcIPJEX4\ngaQIP5AU4QeSunKSK1u3bl3MzMxMcpVAKvPz8zp37pyHWXas8NveLukPkq6Q9KeI2NNv+ZmZGXU6\nnXFWCaCPdrs99LIjH/bbvkLSHyXdJWmLpF22t4z65wGYrHHO+bdJ+iAiPoqILyS9LGlnOWUBqNo4\n4d8o6ZOe96eLtm+xPWu7Y7vT7XbHWB2AMlV+tT8i5iKiHRHtVqtV9eoADGmc8C9I2tTz/vqiDcAU\nGCf870jabPtG21dLuk/S/nLKAlC1kbv6IuKi7V9J+oeWuvr2RsSJ0ioDUKmx+vkj4oCkAyXVAmCC\nuL0XSIrwA0kRfiApwg8kRfiBpAg/kBThB5Ii/EBShB9IivADSRF+ICnCDyRF+IGkCD+QFOEHkiL8\nQFKEH0iK8ANJEX4gKcIPJEX4gaQIP5AU4QeSIvxAUoQfSIrwA0kRfiApwg8kRfiBpMYapdf2vKQL\nkr6UdDEi2mUUheawXdu6I6K2dWcwVvgLP4+IcyX8OQAmiMN+IKlxwx+S3rD9ru3ZMgoCMBnjHvbf\nGhELtn8g6aDtf0fE4d4Fiv8UZiXphhtuGHN1AMoy1p4/IhaK10VJr0natswycxHRjoh2q9UaZ3UA\nSjRy+G1fY/u6r+cl3SnpeFmFAajWOIf96yW9VnQFXSnprxHx91KqAlC5kcMfER9J+kmJtWBEdfbF\nV2nc34v7BPqjqw9IivADSRF+ICnCDyRF+IGkCD+QVBlP9WFMq7Wrrm79tivdgOz5gbQIP5AU4QeS\nIvxAUoQfSIrwA0kRfiAp+vknYJr78evsD5/m7TYN2PMDSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFL0\n85egyf3R0/zc+qDam7zdpwF7fiApwg8kRfiBpAg/kBThB5Ii/EBShB9IamD4be+1vWj7eE/bWtsH\nbZ8qXtdUW2b9bK84VS0iRp6wvH5/n1nuHxhmz/+CpO2XtD0u6VBEbJZ0qHgPYIoMDH9EHJZ0/pLm\nnZL2FfP7JN1dcl0AKjbqOf/6iDhTzH8qaX1J9QCYkLEv+MXSieWKJ5e2Z213bHe63e64qwNQklHD\nf9b2BkkqXhdXWjAi5iKiHRHtVqs14uoAlG3U8O+XtLuY3y3p9XLKATApw3T1vSTpn5J+ZPu07Qck\n7ZF0h+1Tkn5RvAcwRQY+zx8Ru1b46PaSa0mL/vjR8Lz/eLjDD0iK8ANJEX4gKcIPJEX4gaQIP5AU\nX909AXTloYnY8wNJEX4gKcIPJEX4gaQIP5AU4QeSIvxAUoQfSIrwA0kRfiApwg8kRfiBpAg/kBTh\nB5Ii/EBSPM8/JJ7Jx2rDnh9IivADSRF+ICnCDyRF+IGkCD+QFOEHkhoYftt7bS/aPt7T9qTtBdtH\nimlHtWUCl7Pdd0J/w+z5X5C0fZn2ZyJiazEdKLcsAFUbGP6IOCzp/ARqATBB45zzP2z7aHFasKa0\nigBMxKjhf1bSTZK2Sjoj6amVFrQ9a7tju9PtdkdcHYCyjRT+iDgbEV9GxFeSnpO0rc+ycxHRjoh2\nq9UatU4AJRsp/LY39Ly9R9LxlZYF0EwDH+m1/ZKk2ySts31a0m8l3WZ7q6SQNC/pwQprBFCBgeGP\niF3LND9fQS1ooNXaX873M3CHH5AW4QeSIvxAUoQfSIrwA0kRfiApvrp7lVutXXUYH3t+ICnCDyRF\n+IGkCD+QFOEHkiL8QFKEH0iKfv5VgL58jII9P5AU4QeSIvxAUoQfSIrwA0kRfiApwg8kRT//FKiz\nH7/Kr7iu+vfi67n7Y88PJEX4gaQIP5AU4QeSIvxAUoQfSIrwA0kNDL/tTbbftP2+7RO2Hyna19o+\naPtU8bqm+nJXJ9t9pypFRN9pXHX9XhhsmD3/RUmPRcQWST+V9JDtLZIel3QoIjZLOlS8BzAlBoY/\nIs5ExHvF/AVJJyVtlLRT0r5isX2S7q6qSADl+07n/LZnJN0s6W1J6yPiTPHRp5LWl1oZgEoNHX7b\n10p6RdKjEfFZ72exdHK47Ami7VnbHdudbrc7VrEAyjNU+G1fpaXgvxgRrxbNZ21vKD7fIGlxuZ+N\niLmIaEdEu9VqlVEzgBIMc7Xfkp6XdDIinu75aL+k3cX8bkmvl18egKoM80jvLZLul3TM9pGi7QlJ\neyT9zfYDkj6WdG81JWIcVT/WulofN85gYPgj4i1JK/0N315uOQAmhTv8gKQIP5AU4QeSIvxAUoQf\nSIrwA0nx1d2r3DQ/Oks/frXY8wNJEX4gKcIPJEX4gaQIP5AU4QeSIvxAUvTzN8Cg/uxp7qvvh378\nerHnB5Ii/EBShB9IivADSRF+ICnCDyRF+IGk6OefAk2+D4C++unFnh9IivADSRF+ICnCDyRF+IGk\nCD+QFOEHkhoYftubbL9p+33bJ2w/UrQ/aXvB9pFi2lF9uVhORNQ2YXoNc5PPRUmPRcR7tq+T9K7t\ng8Vnz0TE76srD0BVBoY/Is5IOlPMX7B9UtLGqgsDUK3vdM5ve0bSzZLeLpoetn3U9l7ba1b4mVnb\nHdudbrc7VrEAyjN0+G1fK+kVSY9GxGeSnpV0k6StWjoyeGq5n4uIuYhoR0S71WqVUDKAMgwVfttX\naSn4L0bEq5IUEWcj4suI+ErSc5K2VVcmgLINc7Xfkp6XdDIinu5p39Cz2D2SjpdfHoCqDHO1/xZJ\n90s6ZvtI0faEpF22t0oKSfOSHqykQgCVGOZq/1uSlntg/ED55QCYFO7wA5Ii/EBShB9IivADSRF+\nICnCDyRF+IGkCD+QFOEHkiL8QFKEH0iK8ANJEX4gKcIPJOVJfv2y7a6kj3ua1kk6N7ECvpum1tbU\nuiRqG1WZtf0wIob6vryJhv+yldudiGjXVkAfTa2tqXVJ1DaqumrjsB9IivADSdUd/rma199PU2tr\nal0StY2qltpqPecHUJ+69/wAalJL+G1vt/0f2x/YfryOGlZie972sWLk4U7Ntey1vWj7eE/bWtsH\nbZ8qXpcdJq2m2hoxcnOfkaVr3XZNG/F64of9tq+Q9F9Jd0g6LekdSbsi4v2JFrIC2/OS2hFRe5+w\n7Z9J+lzSnyPix0Xb7ySdj4g9xX+cayLi1w2p7UlJn9c9cnMxoMyG3pGlJd0t6Zeqcdv1qete1bDd\n6tjzb5P0QUR8FBFfSHpZ0s4a6mi8iDgs6fwlzTsl7Svm92npH8/ErVBbI0TEmYh4r5i/IOnrkaVr\n3XZ96qpFHeHfKOmTnven1awhv0PSG7bftT1bdzHLWF8Mmy5Jn0paX2cxyxg4cvMkXTKydGO23Sgj\nXpeNC36XuzUitkq6S9JDxeFtI8XSOVuTumuGGrl5UpYZWfobdW67UUe8Llsd4V+QtKnn/fVFWyNE\nxELxuijpNTVv9OGzXw+SWrwu1lzPN5o0cvNyI0urAduuSSNe1xH+dyRttn2j7asl3Sdpfw11XMb2\nNcWFGNm+RtKdat7ow/sl7S7md0t6vcZavqUpIzevNLK0at52jRvxOiImPknaoaUr/h9K+k0dNaxQ\n102S/lVMJ+quTdJLWjoM/J+Wro08IOn7kg5JOiXpDUlrG1TbXyQdk3RUS0HbUFNtt2rpkP6opCPF\ntKPubdenrlq2G3f4AUlxwQ9IivADSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFL/B/dGNK/XTu3WAAAA\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1273fd710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# find incorrectly classified images\n",
    "misclassified = np.where(test_pred != y_test.values.ravel())[0]\n",
    "\n",
    "# choose 2 random misclassified images\n",
    "misclass_imgs = np.random.choice(misclassified, 2)\n",
    "\n",
    "# loop to print info on misclassified images\n",
    "for img in misclass_imgs:\n",
    "    label = y_test.loc[img].values.ravel()\n",
    "    pred = test_pred[img]\n",
    "    print('\\nimage',str(img),'is',str(label[0]),'but was misclassified as',str(pred),':')\n",
    "    plt.imshow(x_test.loc[img].values.reshape((28,28)),cmap=plt.cm.gray_r);\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMM and LDA\n",
    "Now, we will implement Gaussian Mixture Model and Linear Discriminant Anal- ysis on the breast cancer data (sklearn.datasets.load breast cancer) available in sklean.datasets. Load the data and split it into train-validation-test (40-20-40 split). Donâ€™t shuffle the data, otherwise your results will be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (227, 30)\n",
      "x_val shape: (114, 30)\n",
      "x_test shape: (228, 30)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "x = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "\n",
    "# split data into 40% train, 20% validation, 40% test sets\n",
    "#x_train, x_val, x_test = x[:228], x[229:342], x[343:]\n",
    "#y_train, y_val, y_test = y[:228], y[229:342], y[343:]\n",
    "x_train, x_val, x_test = x[:int(0.4*len(y))], x[int(0.4*len(y)):int(0.6*len(y))], x[int(0.6*len(y)):]\n",
    "y_train, y_val, y_test = y[:int(0.4*len(y))], y[int(0.4*len(y)):int(0.6*len(y))], y[int(0.6*len(y)):]\n",
    "\n",
    "\n",
    "# check shapes\n",
    "print('x_train shape:',x_train.shape)\n",
    "print('x_val shape:',x_val.shape)\n",
    "print('x_test shape:',x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6:\n",
    "Implement Gaussian Mixture model on the data as shown in class. Tune the covariance type parameter on the validation data. Use the selected value to compute the test accuracy. As always, train the model on train+validation data to compute the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy for covariance type full = 0.912280701754\n",
      "Validation accuracy for covariance type tied = 0.859649122807\n",
      "Validation accuracy for covariance type diag = 0.947368421053\n",
      "Validation accuracy for covariance type spherical = 0.973684210526\n"
     ]
    }
   ],
   "source": [
    "components = len(np.unique(y_train))\n",
    "\n",
    "for cov in ['full', 'tied', 'diag', 'spherical']:\n",
    "\n",
    "    clf = GaussianMixture(n_components=components, covariance_type=cov, random_state=2)\n",
    "    clf.means_init = np.array([x_train[y_train == i].mean(axis=0)\n",
    "                                for i in range(components)])\n",
    "    clf.fit(x_train, y_train)\n",
    "    pred = clf.predict(x_val)\n",
    "    print ('Validation accuracy for covariance type '+ cov + ' = ' + str(accuracy_score(y_val, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 0.938596491228\n"
     ]
    }
   ],
   "source": [
    "# Best for spherical\n",
    "\n",
    "x_train = np.concatenate((x_train, x_val))\n",
    "y_train = np.concatenate((y_train, y_val))\n",
    "\n",
    "clf = GaussianMixture(n_components=components, covariance_type='spherical', random_state=2)\n",
    "clf.means_init = np.array([x_train[y_train == i].mean(axis=0)\n",
    "                                for i in range(components)])\n",
    "clf.fit(x_train, y_train)\n",
    "pred = clf.predict(x_test)\n",
    "print ('Test accuracy = ' + str(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7:\n",
    "Apply Linear Discriminant Analysis model on the train+validation data and report the accuracy obtained on test data. Report the transformation matrix (w) along with the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy = 0.961876832845\n",
      "Test accuracy = 0.973684210526\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# Intialize\n",
    "lda = LinearDiscriminantAnalysis(store_covariance=True)\n",
    "\n",
    "# Train\n",
    "lda.fit(x_train, y_train)\n",
    "\n",
    "# Test\n",
    "y_test_pred = lda.predict(x_test)\n",
    "y_train_pred = lda.predict(x_train)\n",
    "\n",
    "# print the accuracy\n",
    "print ('Training accuracy = ' + str(np.sum(y_train_pred == y_train)/len(y_train)))\n",
    "print ('Test accuracy = ' + str(np.sum(y_test_pred == y_test)/len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficients:\n",
      " [  5.33168485e+00  -2.12956799e-01  -6.01033070e-01  -8.89253348e-03\n",
      "  -5.44754548e+01   7.43535846e+01  -4.97920818e+00  -3.41702295e+01\n",
      "   1.73163730e+01  -4.44714556e+01  -8.41409536e+00  -2.52828683e-01\n",
      "   5.78531220e-01   1.05895668e-02  -2.80550309e+02  -2.62778874e+00\n",
      "   4.25991726e+01  -1.67347238e+02   1.26610355e+01   2.01114643e+02\n",
      "  -3.54374333e+00  -7.21909964e-02  -5.53734899e-02   2.39371964e-02\n",
      "   1.38875560e+01   1.03981200e+00  -6.05644948e+00  -1.47646877e+01\n",
      "  -1.62743163e+01  -8.04669707e+01]\n"
     ]
    }
   ],
   "source": [
    "print('coefficients:\\n', lda.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept:\n",
      " [ 50.95842876]\n"
     ]
    }
   ],
   "source": [
    "print('intercept:\\n',lda.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Evaluating Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8:\n",
    "Load the digits dataset (scikit-learnâ€™s toy dataset) and take the last 1300 samples as your test set. Train a K-Nearest Neighbor (k=5, linf distance) model and then without using any scikit-learn method, report the final values for Specificity, Sensitivity, TPR, TNR, FNR, FPR, Precision and Recall for Digit 3 (this digit is a positive, everything else is a negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (497, 64)\n",
      "x_test shape: (1300, 64)\n",
      "y_train shape: (497,)\n",
      "y_test shape: (1300,)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "digits = datasets.load_digits()\n",
    "x = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# split off last 1300 samples as test set\n",
    "x_train, x_test = x[:-1300], x[-1300:]\n",
    "y_train, y_test = y[:-1300], y[-1300:]\n",
    "\n",
    "# check shapes\n",
    "print('x_train shape:',x_train.shape)\n",
    "print('x_test shape:',x_test.shape)\n",
    "\n",
    "print('y_train shape:',y_train.shape)\n",
    "print('y_test shape:',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# initialize model\n",
    "knn = KNeighborsClassifier(n_neighbors=5, metric='chebyshev')\n",
    "\n",
    "# fit training data\n",
    "knn.fit(x_train, y_train)\n",
    "\n",
    "# predict\n",
    "train_pred = knn.predict(x_train)\n",
    "test_pred = knn.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find which test set indices were predicted and true 3 values\n",
    "pred_3 = np.where(test_pred == 3)[0]\n",
    "true_3 = np.where(y_test == 3)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 113\n",
      "TN: 1157\n",
      "FP: 13\n",
      "FN: 17\n"
     ]
    }
   ],
   "source": [
    "# calculate TP, FP, TN, FN\n",
    "TP = len([x for x in true_3 if x in pred_3])\n",
    "FN = len([x for x in true_3 if x not in pred_3])\n",
    "FP = len([x for x in pred_3 if x not in true_3])\n",
    "TN = 1300 - TP - FN - FP\n",
    "\n",
    "print('TP:',TP)\n",
    "print('TN:',TN)\n",
    "print('FP:',FP)\n",
    "print('FN:',FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR = sensitivity = recall = TP / (TP + FN) = 0.869\n",
      "TNR = specificity = TN / (FP + TN) = 0.989\n",
      "FPR = 1 - specificity = FP / (FP + TN) = 0.011\n",
      "FNR = 1 - sensitivity = FN / (FN + TP) = 0.131\n",
      "precision = TP / (TP + FP) = 0.897\n",
      "\n",
      "Note:\n",
      "sensitivity = recall = TPR \n",
      "specificity = TNR\n",
      "FPR = 1 - specificity\n",
      "FNR = 1 - sensitivity\n"
     ]
    }
   ],
   "source": [
    "# calculate Specificity, Sensitivity, TPR, TNR, FNR, FPR, Precision and Recall\n",
    "sensitivity = TP / (TP + FN)\n",
    "specificity = TN / (FP + TN)\n",
    "TPR = TP / (TP + FN) # same as sensitivity and recall\n",
    "TNR = TN / (FP + TN) # same as specificity\n",
    "FPR = FP / (FP + TN) # 1 - specificity\n",
    "FNR = FN / (FN + TP) # 1 - FPR\n",
    "recall = TP / (TP + FN) # same as TPR and sensitivity\n",
    "precision = TP / (TP + FP)\n",
    "\n",
    "print('TPR = sensitivity = recall = TP / (TP + FN) = %.3f' % TPR)\n",
    "print('TNR = specificity = TN / (FP + TN) = %.3f' % TNR)\n",
    "print('FPR = 1 - specificity = FP / (FP + TN) = %.3f' % FPR)\n",
    "print('FNR = 1 - sensitivity = FN / (FN + TP) = %.3f' % FNR)\n",
    "print('precision = TP / (TP + FP) = %.3f' % precision)\n",
    "\n",
    "print('\\nNote:')\n",
    "print('sensitivity = recall = TPR ')\n",
    "print('specificity = TNR')\n",
    "print('FPR = 1 - specificity')\n",
    "print('FNR = 1 - sensitivity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Regression\n",
    "An ablation experiment consists of removing one feature from an experiment, in order to assess the amount of additional information that feature provides above and beyond the others. For this section, we will use the diabetes dataset from scikit-learnâ€™s toy datasets. Split the data into training and testing data as a 90-10 split with random state of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "diabetes = datasets.load_diabetes()\n",
    "x = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "# add ones column\n",
    "x = np.c_[x, np.ones(len(x))]\n",
    "\n",
    "# training test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9:\n",
    "Perform least squares regression on this dataset. Report the mean squared error and the mean absolute error on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Least squares regression\n",
    "theta,residuals,rank,s = np.linalg.lstsq(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = np.dot(x_test, theta)\n",
    "\n",
    "# Let's see the output on training data as well, to see the training error\n",
    "y_true_pred = np.dot(x_train, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE test = 2155.96465\n",
      "MAE test = 36.31813\n"
     ]
    }
   ],
   "source": [
    "# MSE and MAE calculation\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print ('MSE test = %.5f' % mean_squared_error(y_test, predictions))\n",
    "print ('MAE test = %.5f' % mean_absolute_error(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10:\n",
    "Repeat the experiment from Question 10 for all possible values of ablation (i.e., removing the feature 1 only, then removing the feature 2 only, and so on). Report all MSEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for model without feature 0 = 2152.80664\n",
      "MSE for model without feature 1 = 2259.13308\n",
      "MSE for model without feature 2 = 2783.51448\n",
      "MSE for model without feature 3 = 2424.77235\n",
      "MSE for model without feature 4 = 2187.59952\n",
      "MSE for model without feature 5 = 2167.51761\n",
      "MSE for model without feature 6 = 2159.15148\n",
      "MSE for model without feature 7 = 2153.06317\n",
      "MSE for model without feature 8 = 2335.17338\n",
      "MSE for model without feature 9 = 2165.86619\n"
     ]
    }
   ],
   "source": [
    "MSEs = []\n",
    "\n",
    "for i in range(x_train.shape[1]-1):\n",
    "    # delete i_th column\n",
    "    x_train_i = np.delete(x_train,i,1)\n",
    "    x_test_i = np.delete(x_test,i,1)\n",
    "    \n",
    "    # create regression model\n",
    "    theta,residuals,rank,s = np.linalg.lstsq(x_train_i, y_train)\n",
    "    \n",
    "    # make predictions on test data\n",
    "    pred_i = np.dot(x_test_i, theta)\n",
    "    MSE_i = mean_squared_error(y_test, pred_i)\n",
    "    MSEs.append(MSE_i)\n",
    "    \n",
    "    # print results\n",
    "    print('MSE for model without feature %i = %.5f' % (i, MSE_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11:\n",
    "Based on the MSE values obtained from Question 11, which features do you deem the most/least significant and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of significant features should result in a worse model, with a higher MSE.  So the most significant features are those that result in the highest MSE when removed.  Conversely, the least significant features should be those that, when removed, result in the lowest MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Most Significant Features: [2 3 8 1 4]\n",
      "MSE for model without feature 2 = 2783.51448185\n",
      "MSE for model without feature 3 = 2424.772348\n",
      "MSE for model without feature 8 = 2335.17338461\n",
      "MSE for model without feature 1 = 2259.13307937\n",
      "MSE for model without feature 4 = 2187.59951938\n",
      "\n",
      "5 Least Significant Features: [0 7 6 9 5]\n",
      "MSE for model without feature 0 = 2152.80664218\n",
      "MSE for model without feature 7 = 2153.06317113\n",
      "MSE for model without feature 6 = 2159.15148251\n",
      "MSE for model without feature 9 = 2165.86619219\n",
      "MSE for model without feature 5 = 2167.51760615\n"
     ]
    }
   ],
   "source": [
    "# get top 5 most and least significant features\n",
    "top5_features = np.argsort(MSEs)[::-1][:5]\n",
    "bottom5_features = np.argsort(MSEs)[:5]\n",
    "\n",
    "print('5 Most Significant Features:',top5_features)\n",
    "for f in top5_features:\n",
    "    print('MSE for model without feature',str(f),'=',str(MSEs[f]))\n",
    "    \n",
    "print('\\n5 Least Significant Features:',bottom5_features)\n",
    "for f in bottom5_features:\n",
    "    print('MSE for model without feature',str(f),'=',str(MSEs[f]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
